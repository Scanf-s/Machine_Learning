{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/CIS-519/coding-hw-dev/blob/master/hw0/solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "NYgS-FIzhkLT"
   },
   "source": [
    "# **SSU CSE 2150508501 2025-1 - Homework 1**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **NOTE**: \"manually graded\" should be submitted along with the written homework solutions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KxzqsGeyrmRG"
   },
   "source": [
    "## **Intro to Python Libraries**\n",
    "In this assignment, we will get acquainted with several important data libraries for machine learning.  The assumption here is that you have already, if necessary, done a thorough review of the basics of python, as in [this series](https://realpython.com/courses/python-turtle-beginners/).\n",
    "\n",
    "\n",
    "## Imports\n",
    "Let's import 2 data-related packages we will use throughout this course: numpy, and pandas."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3fabi-gZs-Bp",
    "ExecuteTime": {
     "end_time": "2025-04-23T06:44:00.291044Z",
     "start_time": "2025-04-23T06:44:00.287910Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "\n",
    "NOTEBOOK = (os.getenv('IS_AUTOGRADER') is None)\n",
    "NOTEBOOK"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFJomUFuH0UV"
   },
   "source": [
    "## Numpy\n",
    "\n",
    "`numpy` is the typical library used to work with arrays.  You can either convert python lists into arrays, or use one of the many libraries which works with arrays, such as `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQnWfo_vILPj"
   },
   "source": [
    "**Question 1a (5 points):** In the following assignment, you will create a function which returns a numpy array using the method np.array(xs), where xs is a list.  The function should take a positive integer n, and return $[e^0 = 1, e^1 = e,..., e^n]$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-sc00FC6JUr-",
    "ExecuteTime": {
     "end_time": "2025-04-23T06:44:02.069978Z",
     "start_time": "2025-04-23T06:44:02.067045Z"
    }
   },
   "source": [
    "def exps_up_to_n(n : int) -> np.array:\n",
    "    '''\n",
    "    inputs: a number n.\n",
    "    outputs: a numpy array X with shape (n + 1), such that for all i between 0 and n (inclusive), X[i] = e^i.\n",
    "    Useful functions are math.exp and range'''\n",
    "    # STUDENT TODO START:\n",
    "    return np.array([math.exp(i) for i in range(0, n + 1)])\n",
    "    # STUDENT TODO END"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "30aCXSNpipyV",
    "ExecuteTime": {
     "end_time": "2025-04-23T06:44:03.920804Z",
     "start_time": "2025-04-23T06:44:03.917792Z"
    }
   },
   "source": [
    "exps_up_to_n(10)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e+00, 2.71828183e+00, 7.38905610e+00, 2.00855369e+01,\n",
       "       5.45981500e+01, 1.48413159e+02, 4.03428793e+02, 1.09663316e+03,\n",
       "       2.98095799e+03, 8.10308393e+03, 2.20264658e+04])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "JBoEKQVL3dpK"
   },
   "source": [
    "Included in every question is one or more test cases that can be used to check if your implementation is likely to satisfy the grader."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zSh5-CS53kuR",
    "ExecuteTime": {
     "end_time": "2025-04-23T06:44:11.559015Z",
     "start_time": "2025-04-23T06:44:11.556432Z"
    }
   },
   "source": [
    "# local test case\n",
    "def test_eval_exps():\n",
    "    \"\"\"We will have hidden values for n in the real gradescope\"\"\"\n",
    "    n = 5\n",
    "    val = map(int, exps_up_to_n(n))\n",
    "    assert(list(val) == [1,2,7,20,54,148])\n",
    "\n",
    "# run local test case only in the notebook\n",
    "if NOTEBOOK:\n",
    "  test_eval_exps()"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "kgWtUx18U3s2"
   },
   "source": [
    "**Question 1b (10 points):** You will find the mean and standard deviation of a numpy array.  Note that this can be done without numpy library functions, although it is much easier with such functions - either version will receive full points if correct.  Partial credit will be awarded if either the mean or standard deviation is computed correctly, but the other is not."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zib-0POKVYPM",
    "ExecuteTime": {
     "end_time": "2025-04-23T06:44:13.491259Z",
     "start_time": "2025-04-23T06:44:13.488781Z"
    }
   },
   "source": [
    "def mean_standard_dev_tuple_from_xs(xs : np.array):\n",
    "    '''\n",
    "    inputs: a numpy array xs\n",
    "    outputs: a tuple (mu, sigma), where mu is the mean of xs, and sigma is the standard deviation of xs.\n",
    "    '''\n",
    "    # STUDENT TODO START:\n",
    "    mean: float = np.mean(xs) # 평균\n",
    "    standard_deviation: float = np.std(xs) # 표준 편차\n",
    "    return mean, standard_deviation\n",
    "    # STUDENT TODO END"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "grHXKLHL3uVg",
    "ExecuteTime": {
     "end_time": "2025-04-23T06:44:15.793907Z",
     "start_time": "2025-04-23T06:44:15.791062Z"
    }
   },
   "source": [
    "def test_mean_standard_dev_tuple_from_xs_1():\n",
    "        \"\"\"We will have hidden values for xs in the real gradescope\"\"\"\n",
    "        xs = [math.exp(i*2.1)/3 for i in range(10)]\n",
    "        assert(int(mean_standard_dev_tuple_from_xs(xs)[0]) == 6134451)\n",
    "\n",
    "def test_mean_standard_dev_tuple_from_xs_2():\n",
    "        \"\"\"We will have hidden values for xs in the real gradescope\"\"\"\n",
    "        xs = [math.exp(i*2.1)/3 for i in range(10)]\n",
    "        assert(int(mean_standard_dev_tuple_from_xs(xs)[1]) == 16017926)\n",
    "\n",
    "# run test cases\n",
    "if NOTEBOOK:\n",
    "  test_mean_standard_dev_tuple_from_xs_1()\n",
    "  test_mean_standard_dev_tuple_from_xs_2()"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FL_ZjHKKBd2"
   },
   "source": [
    "## Pandas\n",
    "Another common way of organizing code is with the Pandas library.  Pandas represents data as DataFrames and Series, where Series are vectors of dara and DataFrames represent labeled sets of Series.  We'll use a dataframe provided by a data collection in a library called seaborn."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "t66VqXMkKmz-",
    "outputId": "e6c4ba91-665d-4484-8ee0-16549dc10ddd",
    "ExecuteTime": {
     "end_time": "2025-04-23T06:44:17.573690Z",
     "start_time": "2025-04-23T06:44:17.559711Z"
    }
   },
   "source": [
    "import seaborn as sns\n",
    "iris = sns.load_dataset('iris')\n",
    "\n",
    "iris"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width    species\n",
       "0             5.1          3.5           1.4          0.2     setosa\n",
       "1             4.9          3.0           1.4          0.2     setosa\n",
       "2             4.7          3.2           1.3          0.2     setosa\n",
       "3             4.6          3.1           1.5          0.2     setosa\n",
       "4             5.0          3.6           1.4          0.2     setosa\n",
       "..            ...          ...           ...          ...        ...\n",
       "145           6.7          3.0           5.2          2.3  virginica\n",
       "146           6.3          2.5           5.0          1.9  virginica\n",
       "147           6.5          3.0           5.2          2.0  virginica\n",
       "148           6.2          3.4           5.4          2.3  virginica\n",
       "149           5.9          3.0           5.1          1.8  virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcGKMc9SNDge"
   },
   "source": [
    "This dataset is one of the most commonly used introductions to data science.  It involves 4 features, which are all real numbers between 0 and some maximum size.  Later, we'll learn how to use those features to predict a label associated with each data point (what type of flower it represents).  For now, we'll learn how to find the subset of the data with a given label."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sEOgNyIeNdtu",
    "ExecuteTime": {
     "end_time": "2025-04-23T06:44:23.331213Z",
     "start_time": "2025-04-23T06:44:23.321567Z"
    }
   },
   "source": [
    "iris.iloc[np.where(iris[\"species\"] == \"setosa\")[0]]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    sepal_length  sepal_width  petal_length  petal_width species\n",
       "0            5.1          3.5           1.4          0.2  setosa\n",
       "1            4.9          3.0           1.4          0.2  setosa\n",
       "2            4.7          3.2           1.3          0.2  setosa\n",
       "3            4.6          3.1           1.5          0.2  setosa\n",
       "4            5.0          3.6           1.4          0.2  setosa\n",
       "5            5.4          3.9           1.7          0.4  setosa\n",
       "6            4.6          3.4           1.4          0.3  setosa\n",
       "7            5.0          3.4           1.5          0.2  setosa\n",
       "8            4.4          2.9           1.4          0.2  setosa\n",
       "9            4.9          3.1           1.5          0.1  setosa\n",
       "10           5.4          3.7           1.5          0.2  setosa\n",
       "11           4.8          3.4           1.6          0.2  setosa\n",
       "12           4.8          3.0           1.4          0.1  setosa\n",
       "13           4.3          3.0           1.1          0.1  setosa\n",
       "14           5.8          4.0           1.2          0.2  setosa\n",
       "15           5.7          4.4           1.5          0.4  setosa\n",
       "16           5.4          3.9           1.3          0.4  setosa\n",
       "17           5.1          3.5           1.4          0.3  setosa\n",
       "18           5.7          3.8           1.7          0.3  setosa\n",
       "19           5.1          3.8           1.5          0.3  setosa\n",
       "20           5.4          3.4           1.7          0.2  setosa\n",
       "21           5.1          3.7           1.5          0.4  setosa\n",
       "22           4.6          3.6           1.0          0.2  setosa\n",
       "23           5.1          3.3           1.7          0.5  setosa\n",
       "24           4.8          3.4           1.9          0.2  setosa\n",
       "25           5.0          3.0           1.6          0.2  setosa\n",
       "26           5.0          3.4           1.6          0.4  setosa\n",
       "27           5.2          3.5           1.5          0.2  setosa\n",
       "28           5.2          3.4           1.4          0.2  setosa\n",
       "29           4.7          3.2           1.6          0.2  setosa\n",
       "30           4.8          3.1           1.6          0.2  setosa\n",
       "31           5.4          3.4           1.5          0.4  setosa\n",
       "32           5.2          4.1           1.5          0.1  setosa\n",
       "33           5.5          4.2           1.4          0.2  setosa\n",
       "34           4.9          3.1           1.5          0.2  setosa\n",
       "35           5.0          3.2           1.2          0.2  setosa\n",
       "36           5.5          3.5           1.3          0.2  setosa\n",
       "37           4.9          3.6           1.4          0.1  setosa\n",
       "38           4.4          3.0           1.3          0.2  setosa\n",
       "39           5.1          3.4           1.5          0.2  setosa\n",
       "40           5.0          3.5           1.3          0.3  setosa\n",
       "41           4.5          2.3           1.3          0.3  setosa\n",
       "42           4.4          3.2           1.3          0.2  setosa\n",
       "43           5.0          3.5           1.6          0.6  setosa\n",
       "44           5.1          3.8           1.9          0.4  setosa\n",
       "45           4.8          3.0           1.4          0.3  setosa\n",
       "46           5.1          3.8           1.6          0.2  setosa\n",
       "47           4.6          3.2           1.4          0.2  setosa\n",
       "48           5.3          3.7           1.5          0.2  setosa\n",
       "49           5.0          3.3           1.4          0.2  setosa"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5.2</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>5.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>5.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>5.3</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ivBWrSMQO6f1"
   },
   "source": [
    "**Question 2 (10 points)**: Your assignment is to write a function which takes a flower class, one of either \"setosa\", \"versicolor,\" or \"virginica,\" and a number $n$, and return the mean sepal length of the top n flowers of that class, sorted by sepal width (so the mean sepal length of the n flowers with the highest sepal width out of all flowers in the class.)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yl9bbuNTPcBl",
    "ExecuteTime": {
     "end_time": "2025-04-23T06:44:52.210135Z",
     "start_time": "2025-04-23T06:44:52.207423Z"
    }
   },
   "source": [
    "def get_mean_sepal_length_from_class_top_n_width(iris, flower_class:str, n:int) -> float:\n",
    "    '''\n",
    "    inputs: the iris dataset, a flower_class string and a positive integer\n",
    "    output: a floating point number\n",
    "    using the iris dataset provided above as a pandas DataFrame,\n",
    "    a) obtain only those datapoints which correspond to the given class\n",
    "    b) find the top n of that subset according to sepal width\n",
    "    c) return the mean sepal length of that smaller subset\n",
    "    '''\n",
    "    # STUDENT TODO START:\n",
    "    filtered: np.array = iris.iloc[np.where(iris[\"species\"] == flower_class)[0]] # \"species\"열에서 flower_class에 해당하는 값만 필터링\n",
    "    sorted_by_sepal_width_top_n: np.array = filtered.sort_values(by=[\"sepal_width\"], ascending=False)[:n] # np.sort_values를 통해 \"sepal_width\"열을 기준으로 내림차순 정렬 후, list slicing을 통해 상위 n개만 추출\n",
    "    return np.mean(sorted_by_sepal_width_top_n[\"sepal_length\"]) # 평균 계산해서 반환\n",
    "    # STUDENT TODO END"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6CXO4RGi4GlE",
    "ExecuteTime": {
     "end_time": "2025-04-23T06:44:53.805851Z",
     "start_time": "2025-04-23T06:44:53.802350Z"
    }
   },
   "source": [
    "def test_get_mean():\n",
    "    \"\"\"We will have hidden values for species and n in the real gradescope\"\"\"\n",
    "    n = 5\n",
    "    species = \"setosa\"\n",
    "    species_top_n = [5.7, 5.5, 5.2, 5.8, 5.4]\n",
    "    species_mean = round(get_mean_sepal_length_from_class_top_n_width(iris, species, n), 2)\n",
    "    assert (species_mean == 5.52)\n",
    "\n",
    "# run test case\n",
    "if NOTEBOOK:\n",
    "  test_get_mean()"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OI9SfQ3fRqJ8"
   },
   "source": [
    "**Question 3 (10 points):** Your next task is to take a subset of the iris dataset, which will only have the first n examples of each species; then return a copy of the modified subset that has an integer-valued instead of a string-valued \"species\" column. 0 corresponds to \"setosa\", 1 corresponds to \"versicolor\", and 2 corresponds to \"virginica\".  As we will learn, using \"integer-based categorical representations\" is quite useful in many classification tasks and other types of tasks involving variables associated with different categories."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AbKmUdYsSRQj",
    "ExecuteTime": {
     "end_time": "2025-04-23T06:45:12.282194Z",
     "start_time": "2025-04-23T06:45:12.277927Z"
    }
   },
   "source": [
    "def string_to_categorical_int(iris, n : int) -> pd.DataFrame:\n",
    "    '''\n",
    "    input: the iris dataset, a positive integer n\n",
    "    output: a new dataframe df2 which contains the same data as df except\n",
    "    a) only the first n examples of each species are retained\n",
    "    b) the species labels are converted from strings to integers between 0 and 2 according to the above scheme\n",
    "    '''\n",
    "    # STUDENT TODO START:\n",
    "    classified_setosa: np.array = iris.iloc[np.where(iris[\"species\"] == \"setosa\")[0]][:n]\n",
    "    classified_versicolor: np.array = iris.iloc[np.where(iris[\"species\"] == \"versicolor\")[0]][:n]\n",
    "    classified_virginica: np.array = iris.iloc[np.where(iris[\"species\"] == \"virginica\")[0]][:n]\n",
    "\n",
    "    df2 = pd.concat([classified_setosa, classified_versicolor, classified_virginica]) # pd.concat을 통해 np.array 합칠 수 있음\n",
    "    df2[\"species\"] = df2[\"species\"].map({\"setosa\": 0, \"versicolor\": 1, \"virginica\": 2}) # pd.concat을 통해 특정 열에에 대해 특정 값을 mapping할 수 있음\n",
    "    return df2\n",
    "    # STUDENT TODO END"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "B4pySlZ14OVn",
    "ExecuteTime": {
     "end_time": "2025-04-23T06:45:13.799898Z",
     "start_time": "2025-04-23T06:45:13.792013Z"
    }
   },
   "source": [
    "def test_string_to_cat1():\n",
    "        \"\"\"We will have hidden values for species, n and column in the real gradescope\"\"\"\n",
    "        n = 5\n",
    "        df2 = string_to_categorical_int(iris, n)\n",
    "        species = \"setosa\"\n",
    "        species_cat = {\"setosa\":0, \"versicolor\":1, \"virginica\":2}[species]\n",
    "        column = \"species\"\n",
    "        assert (df2[column].tolist() == [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2])\n",
    "\n",
    "def test_string_to_cat2():\n",
    "        n = 5\n",
    "        \"\"\"We will have hidden values for species, n and column in the real gradescope\"\"\"\n",
    "        df2 = string_to_categorical_int(iris, n)\n",
    "        species = \"setosa\"\n",
    "        species_cat = {\"setosa\":0, \"versicolor\":1, \"virginica\":2}[species]\n",
    "        column = \"sepal_length\"\n",
    "        column_vals_df2 = [5.1, 4.9, 4.7, 4.6, 5.0, 7.0, 6.4, 6.9, 5.5, 6.5, 6.3, 5.8, 7.1, 6.3, 6.5]\n",
    "        assert (df2[column].tolist() == column_vals_df2)\n",
    "\n",
    "def test_string_to_cat3():\n",
    "        \"\"\"We will have hidden values for n in the real gradescope\"\"\"\n",
    "        n = 5\n",
    "        df2 = string_to_categorical_int(iris, n)\n",
    "        assert (np.all(df2.columns == iris.columns))\n",
    "\n",
    "# run test cases\n",
    "if NOTEBOOK:\n",
    "  test_string_to_cat1()\n",
    "  test_string_to_cat2()\n",
    "  test_string_to_cat3()"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Datasets**\n",
    "Next, we will download all datasets from Google Drive to your local runtime. After successful download, you may verify that all datasets are present in your colab instance or your run directory.\n",
    "\n",
    "HW1 Datasets:\n",
    "- [CSE-SSU_hw1_diabetes_train.csv](https://drive.google.com/file/d/1w9_4pyJuG9KC3cg00ILhZRdkjqhEuNtD/view?usp=sharing)\n",
    "- [CSE-SSU_hw1_diabetes_test.csv](https://drive.google.com/file/d/1W0kVVfR1DIGxgmH_oY1MBpPMcPslR8T8/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T06:45:17.810347Z",
     "start_time": "2025-04-23T06:45:17.791281Z"
    }
   },
   "source": [
    "train_df = pd.read_csv(\"CSE-SSU_hw1_diabetes_train.csv\")\n",
    "train_df"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      OHQ575Q  INDFMPIR  LBDSGBSI  OHQ565  DIABETIC  OHX28CSC  OHX19SE  \\\n",
       "0         NaN      1.52      28.0     NaN         0       NaN      0.0   \n",
       "1         NaN      3.09      28.0     NaN         0       NaN      NaN   \n",
       "2         NaN      5.00      29.0     NaN         0       NaN      NaN   \n",
       "3         NaN      5.00       NaN     NaN         0       NaN      NaN   \n",
       "4         NaN      2.13      31.0     NaN         0       NaN      NaN   \n",
       "...       ...       ...       ...     ...       ...       ...      ...   \n",
       "8135      NaN       NaN       NaN     NaN         0       NaN      NaN   \n",
       "8136      NaN      1.29      31.0     NaN         0       NaN      NaN   \n",
       "8137      NaN       NaN       NaN     NaN         0       NaN      0.0   \n",
       "8138      NaN       NaN       NaN     NaN         0       NaN      NaN   \n",
       "8139      NaN      0.02      29.0     2.0         0       6.0      0.0   \n",
       "\n",
       "      URDECPLC  BMXBMI  BMXWT  ...  DMDEDUC2  RIDAGEYR  LBXTC  ALQ120Q  \\\n",
       "0          0.0    20.2   59.2  ...       NaN        18  154.0      NaN   \n",
       "1          NaN    21.1   61.8  ...       3.0        57  224.0      1.0   \n",
       "2          NaN    22.1   63.1  ...       4.0        29  170.0      2.0   \n",
       "3          NaN     NaN    6.8  ...       NaN         0    NaN      NaN   \n",
       "4          0.0    21.7   65.8  ...       3.0        61  115.0      7.0   \n",
       "...        ...     ...    ...  ...       ...       ...    ...      ...   \n",
       "8135       NaN     NaN    NaN  ...       NaN         2    NaN      NaN   \n",
       "8136       0.0    30.6   87.9  ...       1.0        79  184.0      0.0   \n",
       "8137       0.0    31.7   90.4  ...       NaN        17    NaN      NaN   \n",
       "8138       NaN     NaN    NaN  ...       2.0        58    NaN      NaN   \n",
       "8139       NaN    16.1   33.5  ...       NaN        12  123.0      NaN   \n",
       "\n",
       "      BMXWAIST  BPQ020  RIDRETH1  BMXLEG  RIAGENDR  BMXHT  \n",
       "0         75.5     2.0         5    42.0         1  171.0  \n",
       "1         90.7     2.0         2    39.0         2  171.0  \n",
       "2         81.2     2.0         2    40.0         2  169.0  \n",
       "3          NaN     NaN         1     NaN         2    NaN  \n",
       "4         77.2     1.0         3    41.5         1  174.2  \n",
       "...        ...     ...       ...     ...       ...    ...  \n",
       "8135       NaN     NaN         5     NaN         2    NaN  \n",
       "8136     117.5     2.0         1    39.0         1  169.4  \n",
       "8137     108.7     2.0         4    41.7         2  168.9  \n",
       "8138       NaN     2.0         4     NaN         1    NaN  \n",
       "8139      58.3     NaN         4    34.8         1  144.2  \n",
       "\n",
       "[8140 rows x 21 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OHQ575Q</th>\n",
       "      <th>INDFMPIR</th>\n",
       "      <th>LBDSGBSI</th>\n",
       "      <th>OHQ565</th>\n",
       "      <th>DIABETIC</th>\n",
       "      <th>OHX28CSC</th>\n",
       "      <th>OHX19SE</th>\n",
       "      <th>URDECPLC</th>\n",
       "      <th>BMXBMI</th>\n",
       "      <th>BMXWT</th>\n",
       "      <th>...</th>\n",
       "      <th>DMDEDUC2</th>\n",
       "      <th>RIDAGEYR</th>\n",
       "      <th>LBXTC</th>\n",
       "      <th>ALQ120Q</th>\n",
       "      <th>BMXWAIST</th>\n",
       "      <th>BPQ020</th>\n",
       "      <th>RIDRETH1</th>\n",
       "      <th>BMXLEG</th>\n",
       "      <th>RIAGENDR</th>\n",
       "      <th>BMXHT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.52</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>59.2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18</td>\n",
       "      <td>154.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1</td>\n",
       "      <td>171.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.09</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.1</td>\n",
       "      <td>61.8</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>57</td>\n",
       "      <td>224.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>90.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2</td>\n",
       "      <td>171.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.00</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.1</td>\n",
       "      <td>63.1</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>29</td>\n",
       "      <td>170.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>81.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2</td>\n",
       "      <td>169.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.8</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.13</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.7</td>\n",
       "      <td>65.8</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>61</td>\n",
       "      <td>115.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>77.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>41.5</td>\n",
       "      <td>1</td>\n",
       "      <td>174.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8135</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8136</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.29</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.6</td>\n",
       "      <td>87.9</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>79</td>\n",
       "      <td>184.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "      <td>169.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8137</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.7</td>\n",
       "      <td>90.4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>41.7</td>\n",
       "      <td>2</td>\n",
       "      <td>168.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8138</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8139</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.1</td>\n",
       "      <td>33.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>123.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>34.8</td>\n",
       "      <td>1</td>\n",
       "      <td>144.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8140 rows × 21 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Logistic Regression**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.1. Logistic Regression Implementation [18 pts]**\n",
    "\n",
    "Implement logistic regression with both L1 and L2 regularization by completing the LogisticRegression class.  \n",
    "\n",
    "Your class must implement the following API:\n",
    "\n",
    "* `__init__(alpha, tol, max_iter, theta_init, penalty, lambd)`\n",
    "* `sigmoid(x)`\n",
    "* `compute_cost(theta, X, y)`\n",
    "* `compute_gradient(theta, X, y)`\n",
    "* `has_converged(theta_old, theta_new)`\n",
    "* `fit(X, y)`\n",
    "* `predict_proba(X)`\n",
    "* `predict(X)`\n",
    "\n",
    "Note that these methods have already been defined correctly for you in the LogisticRegression class. **DO NOT** change the API.\n",
    "\n",
    "---\n",
    "\n",
    "### **4.1.1. Sigmoid Function [1 pt]**\n",
    "\n",
    "You should begin by implementing the `sigmoid` function.  As you may know, the sigmoid function $\\sigma(x)$ is mathematically defined as follows.\n",
    "\n",
    "> $\\sigma(x) = \\frac{1}{1\\ +\\ \\text{exp}(-x)}$\n",
    "\n",
    "**Be certain that your sigmoid function works with both vectors and matrices** --- for either a vector or a matrix, you function should perform the sigmoid function on every element.\n",
    "\n",
    "---\n",
    "\n",
    "### **4.1.2. Cost Function [5 pts]**\n",
    "\n",
    "The `compute_cost` function should compute the cost for a given $\\theta$ vector. The cost is a scalar value given by:\n",
    "\n",
    "> $\n",
    "\\mathcal{L}({\\theta}) = -\\sum_{i =1}^N [ y_i\\log(h_{{\\theta}}({x}_i)) + (1 - y_i)\\log(1 - h_{{\\theta}}({x}_i))]\n",
    "$\n",
    "\n",
    "where\n",
    "> $\n",
    "h_{\\theta}(x_{i}) = \\sigma(\\theta^{T}x_{i})\n",
    "$\n",
    "\n",
    "\n",
    "L1 Regularisation Loss:\n",
    ">$\n",
    "\\mathcal{L1}({\\theta}) = \\mathcal{L}({\\theta}) + \\lambda \\sum_{j = 1}^D  |{\\theta}_j|\n",
    "$\n",
    "\n",
    "L2 Regularisation Loss:\n",
    ">$\n",
    "\\mathcal{L2}({\\theta}) = \\mathcal{L}({\\theta}) + \\lambda \\sum_{j = 1}^D  {\\theta}_j^2\n",
    "$\n",
    "\n",
    "$N$ is the number of training samples and $D$ is the number of features (excluding the intercept term). $\\theta$ is a $D + 1$ dimensional vector, with the first element being the intercept term. Note that we do not include the intercept in the regularization terms.\n",
    "\n",
    "---\n",
    "\n",
    "### **4.1.3. Gradient of the Cost Function [5 pts]**\n",
    "\n",
    "The `compute_gradient` function should compute the gradient of the cost function at a given $\\theta$.\n",
    "\n",
    "---\n",
    "\n",
    "### **4.1.4. Convergence Check [1 pt]**\n",
    "\n",
    "The `has_converged` function should return whether gradient descent algorithm has converged or not.\n",
    "\n",
    "---\n",
    "\n",
    "### **4.1.5. Training [3 pts]**\n",
    "\n",
    "The `fit` method should train the model via gradient descent, relying on the cost and gradient functions. The trained weights/coefficients must be stored as `theta_`. The weights start as a zero vector. The weights and the corresponding cost after every gradient descent iteration must be stored in `hist_theta_` and `hist_cost_` respectively.\n",
    "\n",
    "* The gradient descent stops or converges when $\\theta$ stops changing or changes negligibly between consecutive iterations, i.e., when\n",
    "$\\| {\\theta}_\\mathit{new} -  {\\theta}_\\mathit{old} \\|_2 \\leq \\epsilon$,\n",
    "for some small $\\epsilon$ (e.g., $\\epsilon$ = 1E-4). $\\epsilon$ is stored as `tol` (short for tolerance).\n",
    "\n",
    "* To ensure that the function terminates, we should set a maximum limit for the number of gradient descent iterations irrespective of whether $\\theta$ converges or not. The limit is stored as `max_iter`.\n",
    "\n",
    "* `alpha` is the learning rate of the gradient descent algorithm.\n",
    "\n",
    "---\n",
    "\n",
    "### **4.1.6. Predict Probability [1 pt]**\n",
    "\n",
    "The `predict_probability` function should predict the probabilities that the data points in a given input data matrix belong to class 1.\n",
    "\n",
    "---\n",
    "\n",
    "### **4.1.7. Predict [2 pts]**\n",
    "\n",
    "The `predict` function should predict the classes of the data points in a given input data matrix."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T08:35:27.829789Z",
     "start_time": "2025-04-23T08:35:27.816869Z"
    }
   },
   "source": [
    "class LogisticRegression:\n",
    "\n",
    "    \"\"\"\n",
    "    Logistic Regression (aka logit, MaxEnt) classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha: float, default=0.01\n",
    "        Learning rate\n",
    "    tol : float, default=0.0001\n",
    "        Tolerance for stopping criteria\n",
    "    max_iter : int, default=10000\n",
    "        Maximum number of iterations of gradient descent\n",
    "    theta_init: None (or) numpy.ndarray of shape (D + 1,)\n",
    "        The initial weights; if None, all weights will be zero by default\n",
    "    penalty : string, default = None\n",
    "        The type of regularization. The other acceptable options are l1 and l2\n",
    "    lambd : float, default = 1.0\n",
    "        The parameter regularisation constant (i.e. lambda)\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    theta_ : numpy.ndarray of shape (D + 1,)\n",
    "        The value of the coefficients after gradient descent has converged\n",
    "        or the number of iterations hit the maximum limit\n",
    "    hist_theta_ : numpy.ndarray of shape (num_iter, D + 1) where num_iter is the number of gradient descent iterations\n",
    "        Stores theta_ after every gradient descent iteration\n",
    "    hist_cost_ : numpy.ndarray of shape (num_iter,) where num_iter is the number of gradient descent iterations\n",
    "        Stores cost after every gradient descent iteration\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=0.01, tol=0.0001, max_iter=10000, theta_init=None, penalty = None, lambd = 1.0):\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.theta_init = theta_init\n",
    "        self.penalty = penalty\n",
    "        self.lambd = lambd\n",
    "        self.theta_ = None\n",
    "        self.hist_cost_ = None\n",
    "        self.hist_theta_ = None\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        # a function needed for using cross_val_score function from sklearn.model_selection\n",
    "        return {\"alpha\": self.alpha, \"max_iter\": self.max_iter, \"lambd\" : self.lambd, \"penalty\" : self.penalty}\n",
    "\n",
    "    def sigmoid(self, x: np.ndarray) -> np.ndarray:\n",
    "\n",
    "        \"\"\"\n",
    "        Compute the sigmoid value of the argument.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: numpy.ndarray\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out: numpy.ndarray\n",
    "            The sigmoid value of x\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO START: Complete the function\n",
    "\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "        # TODO END\n",
    "\n",
    "    def compute_cost(self, theta, X, y) -> float:\n",
    "\n",
    "        \"\"\"\n",
    "        Compute the cost/objective function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        theta: numpy.ndarray of shape (D + 1,)\n",
    "            The coefficients\n",
    "        X: numpy.ndarray of shape (N, D + 1)\n",
    "            The features matrix\n",
    "        y: numpy.ndarray of shape (N,)\n",
    "            The target variable array\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cost: float\n",
    "            The cost as a scalar value\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO START: Complete the function (should account for three cases - no penalty, l1 penalty, and l2 penalty)\n",
    "        # DO NOT use np.dot for this function as it can possibly return nan. Use a combination of np.nansum and np.multiply.\n",
    "\n",
    "        # Cost\n",
    "        h = self.sigmoid(np.nansum(X*theta, axis=1))\n",
    "        cost = -np.nansum(\n",
    "                y * np.log(h) # axis=1 옵션을 주어서 각 열방향으로 합을 구함 -> sigma(theta^T * x_i)\n",
    "                +\n",
    "                (1 - y) * np.log(1 - h)\n",
    "            )\n",
    "\n",
    "        # Regularization\n",
    "        if self.penalty is None:\n",
    "            pass\n",
    "        elif self.penalty == \"l1\": # L1 Regularization\n",
    "            cost += self.lambd * np.nansum(abs(theta[1:])) # 맨 앞에 편향때문에 추가한 데이터 제외\n",
    "        elif self.penalty == \"l2\": # L2 Regularization\n",
    "            cost += self.lambd * np.nansum(np.power(theta[1:], 2))\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected penalty value. choose one of these options : l1, l2, or DO NOT INPUT\")\n",
    "\n",
    "        return cost\n",
    "\n",
    "        # TODO END\n",
    "\n",
    "    def compute_gradient(self, theta, X, y) -> np.ndarray:\n",
    "\n",
    "        \"\"\"\n",
    "        Compute the gradient of the cost function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        theta: numpy.ndarray of shape (D + 1,)\n",
    "            The coefficients\n",
    "        X: numpy.ndarray of shape (N, D + 1)\n",
    "            The features matrix\n",
    "        y: numpy.ndarray of shape (N,)\n",
    "            The target variable array\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradient: numpy.ndarray of shape (D + 1,)\n",
    "            The gradient values\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO START: Complete the function (should account for three cases - no penalty, l1 penalty, and l2 penalty)\n",
    "\n",
    "        h: np.ndarray = self.sigmoid(np.nansum(X*theta, axis=1))\n",
    "        gradient = np.nansum(X * (h - y)[:, None], axis=0) # 특성마다 하나씩 gradient를 구해야 하니까 axis=0 옵션을 줌 (각 특성마다 가중치)\n",
    "\n",
    "        if self.penalty is None:\n",
    "            pass\n",
    "        elif self.penalty == \"l1\": # L1 Regularization\n",
    "            gradient[1:] += self.lambd * np.sign(theta[1:])\n",
    "        elif self.penalty == \"l2\": # L2 Regularization\n",
    "            gradient[1:] += 2 * self.lambd * theta[1:]\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected penalty value. choose one of these options : l1, l2, or DO NOT INPUT\")\n",
    "\n",
    "        return gradient\n",
    "        # TODO END\n",
    "\n",
    "    def has_converged(self, theta_old, theta_new) -> bool:\n",
    "\n",
    "        \"\"\"\n",
    "        Return whether gradient descent has converged.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        theta_old: numpy.ndarray of shape (D + 1,)\n",
    "            The weights prior to the update by gradient descent\n",
    "        theta_new: numpy.ndarray of shape (D + 1,)\n",
    "            The weights after the update by gradient descent\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        converged: bool\n",
    "            Whether gradient descent converged or not\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO START: Complete the function\n",
    "\n",
    "        # 경사 하강법의 종료 조건\n",
    "        # 1. max_iter까지 반복 횟수가 도달한 경우\n",
    "        # 2. 변화량이 tol보다 작은 경우\n",
    "        # 3. 에러값이 충분히 작은 경우\n",
    "        # 여기서는 수렴했는지만 따지니까 2번만 확인\n",
    "\n",
    "        # np.linalg.norm는 두 벡터간의 Norm을 계산하는 함수\n",
    "        # ord=2는 L2 norm을 의미함\n",
    "        return np.linalg.norm(theta_old - theta_new, ord=2) < self.tol\n",
    "\n",
    "        # TODO END\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        \"\"\"\n",
    "        Compute the coefficients using gradient descent and store them as theta_.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy.ndarray of shape (N, D)\n",
    "            The features matrix\n",
    "        y: numpy.ndarray of shape (N,)\n",
    "            The target variable array\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Nothing\n",
    "        \"\"\"\n",
    "\n",
    "        N, D = X.shape\n",
    "\n",
    "        # Adding a column of ones at the beginning for the bias term\n",
    "        ones_col = np.ones((N, 1))\n",
    "        X = np.hstack((ones_col, X))\n",
    "\n",
    "        # Initializing the weights\n",
    "        if self.theta_init is None:\n",
    "            theta_old = np.zeros((D + 1,))\n",
    "        else:\n",
    "            theta_old = self.theta_init\n",
    "\n",
    "        # Initializing the historical weights matrix\n",
    "        # Remember to append this matrix with the weights after every gradient descent iteration\n",
    "        self.hist_theta_ = [theta_old]\n",
    "\n",
    "        # Computing the cost for the initial weights\n",
    "        cost = self.compute_cost(theta_old, X, y)\n",
    "\n",
    "        # Initializing the historical cost array\n",
    "        # Remember to append this array with the cost after every gradient descent iteration\n",
    "        self.hist_cost_ = [cost]\n",
    "\n",
    "        # TODO START: Complete the function\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            gradient = self.compute_gradient(theta_old, X, y) # compute gradient\n",
    "            theta_new = theta_old - self.alpha * gradient # update weights\n",
    "\n",
    "            # hist_theta_에 theta_new 추가\n",
    "            self.hist_theta_.append(theta_new.copy())\n",
    "            # hist_cost_에 cost 추가\n",
    "            self.hist_cost_.append(self.compute_cost(theta_new, X, y))\n",
    "\n",
    "            if self.has_converged(theta_old, theta_new):\n",
    "                theta_old = theta_new\n",
    "                break # stop if converged\n",
    "\n",
    "            theta_old = theta_new\n",
    "\n",
    "        # 최종 결과 저장\n",
    "        self.theta_ = theta_old\n",
    "        self.hist_theta_ = np.vstack(self.hist_theta_)  # np.array를 np.ndarray로 변환해야함 (클래스 주석)\n",
    "        self.hist_cost_ = np.asarray(self.hist_cost_) # np.array를 np.ndarray로 변환해야함 (클래스 주석)\n",
    "\n",
    "        # TODO END\n",
    "\n",
    "    def predict_proba(self, X) -> np.ndarray:\n",
    "\n",
    "        \"\"\"\n",
    "        Predict the probabilities that the data points in X belong to class 1.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy.ndarray of shape (N, D)\n",
    "            The features matrix\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_hat: numpy.ndarray of shape (N,)\n",
    "            The predicted probabilities that the data points in X belong to class 1\n",
    "        \"\"\"\n",
    "\n",
    "        N = X.shape[0]\n",
    "        X = np.hstack((np.ones((N, 1)), X))\n",
    "\n",
    "        # TODO START: Complete the function\n",
    "\n",
    "        # X가 주어졌을 때, y=1일 확률\n",
    "        return self.sigmoid(np.nansum(self.theta_ * X, axis=1))\n",
    "\n",
    "        # TODO END\n",
    "\n",
    "    def predict(self, X) -> np.ndarray:\n",
    "\n",
    "        \"\"\"\n",
    "        Predict the classes of the data points in X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy.ndarray of shape (N, D)\n",
    "            The features matrix\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred: numpy.ndarray of shape (N,)\n",
    "            The predicted class of the data points in X\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO START: Complete the function\n",
    "\n",
    "        return (self.predict_proba(X) >= 0.5).astype(int)\n",
    "\n",
    "        # TODO END"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T08:35:28.740838Z",
     "start_time": "2025-04-23T08:35:28.737348Z"
    }
   },
   "source": [
    "def test_log_reg_sigmoid(StudentLogisticRegression):\n",
    "\n",
    "    student_lr_clf = StudentLogisticRegression()\n",
    "    test_case = np.array([ 1.62434536, -0.61175641, -0.52817175, -1.07296862,  0.86540763])\n",
    "    student_ans = student_lr_clf.sigmoid(test_case)\n",
    "    required_ans = np.array([0.83539354, 0.35165864, 0.3709434 , 0.25483894, 0.70378922])\n",
    "\n",
    "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
    "\n",
    "test_log_reg_sigmoid(LogisticRegression)"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T08:35:29.232005Z",
     "start_time": "2025-04-23T08:35:29.227324Z"
    }
   },
   "source": [
    "def test_log_reg_compute_cost(StudentLogisticRegression):\n",
    "\n",
    "    test_case_theta = np.array([ 1.62434536, -0.61175641])\n",
    "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
    "                            [-0.52817175, -1.07296862],\n",
    "                            [ 0.86540763, -2.3015387 ],\n",
    "                            [ 1.74481176, -0.7612069 ],\n",
    "                            [ 0.3190391,  -0.24937038]])\n",
    "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
    "\n",
    "    student_lr_clf = StudentLogisticRegression()\n",
    "    student_ans = student_lr_clf.compute_cost(test_case_theta, test_case_X, test_case_y)\n",
    "    required_ans = 7.467975765663204\n",
    "\n",
    "    assert np.abs(student_ans - required_ans) <= 1e-2\n",
    "\n",
    "    student_lr_clf = StudentLogisticRegression(penalty=\"l1\", lambd=0.1)\n",
    "    student_ans = student_lr_clf.compute_cost(test_case_theta, test_case_X, test_case_y)\n",
    "    required_ans = 7.52915138076548\n",
    "\n",
    "    assert np.abs(student_ans - required_ans) <= 1e-2\n",
    "\n",
    "    student_lr_clf = StudentLogisticRegression(penalty=\"l2\", lambd=0.1)\n",
    "    student_ans = student_lr_clf.compute_cost(test_case_theta, test_case_X, test_case_y)\n",
    "    required_ans = 7.505400330283089\n",
    "    assert np.abs(student_ans - required_ans) <= 1e-2\n",
    "\n",
    "test_log_reg_compute_cost(LogisticRegression)"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T08:35:29.674347Z",
     "start_time": "2025-04-23T08:35:29.670315Z"
    }
   },
   "source": [
    "def test_log_reg_compute_gradient(StudentLogisticRegression):\n",
    "\n",
    "    test_case_theta = np.array([ 1.62434536, -0.61175641])\n",
    "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
    "                            [-0.52817175, -1.07296862],\n",
    "                            [ 0.86540763, -2.3015387 ],\n",
    "                            [ 1.74481176, -0.7612069 ],\n",
    "                            [ 0.3190391,  -0.24937038]])\n",
    "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
    "\n",
    "    student_lr_clf = StudentLogisticRegression()\n",
    "    student_ans = student_lr_clf.compute_gradient(test_case_theta, test_case_X, test_case_y)\n",
    "    required_ans = np.array([ 2.60573737, -2.20203139])\n",
    "\n",
    "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
    "\n",
    "    student_lr_clf = StudentLogisticRegression(penalty=\"l1\", lambd=0.1)\n",
    "    student_ans = student_lr_clf.compute_gradient(test_case_theta, test_case_X, test_case_y)\n",
    "    required_ans = np.array([ 2.60573737, -2.30203139])\n",
    "\n",
    "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
    "\n",
    "    student_lr_clf = StudentLogisticRegression(penalty=\"l2\", lambd=0.1)\n",
    "    student_ans = student_lr_clf.compute_gradient(test_case_theta, test_case_X, test_case_y)\n",
    "    required_ans = np.array([ 2.60573737, -2.32438267])\n",
    "\n",
    "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
    "\n",
    "test_log_reg_compute_gradient(LogisticRegression)"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T08:35:30.435524Z",
     "start_time": "2025-04-23T08:35:30.431899Z"
    }
   },
   "source": [
    "def test_log_reg_has_converged(StudentLogisticRegression):\n",
    "\n",
    "    student_lr_clf = StudentLogisticRegression()\n",
    "    test_case_theta_old = np.array([ 1.62434536, -0.61175641])\n",
    "    test_case_theta_new = np.array([1.624345, -0.611756])\n",
    "    student_ans = student_lr_clf.has_converged(test_case_theta_old, test_case_theta_new)\n",
    "    required_ans = True\n",
    "\n",
    "    assert student_ans == required_ans\n",
    "\n",
    "test_log_reg_has_converged(LogisticRegression)"
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T08:35:31.548553Z",
     "start_time": "2025-04-23T08:35:31.543675Z"
    }
   },
   "source": [
    "def test_log_reg_fit(StudentLogisticRegression):\n",
    "\n",
    "    student_lr_clf = StudentLogisticRegression(max_iter=5)\n",
    "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
    "                            [-0.52817175, -1.07296862],\n",
    "                            [ 0.86540763, -2.3015387 ],\n",
    "                            [ 1.74481176, -0.7612069 ],\n",
    "                            [ 0.3190391,  -0.24937038]])\n",
    "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
    "    student_lr_clf.fit(test_case_X, test_case_y)\n",
    "    student_ans = student_lr_clf.hist_theta_\n",
    "    required_ans = np.array([[ 0.        ,  0.        ,  0.        ],\n",
    "                             [ 0.005     , -0.00597503,  0.00564325],\n",
    "                             [ 0.01006813, -0.01184464,  0.0111865 ],\n",
    "                             [ 0.01520121, -0.01761226,  0.01663348],\n",
    "                             [ 0.02039621, -0.02328121,  0.02198778],\n",
    "                             [ 0.02565018, -0.0288547 ,  0.02725288]])\n",
    "\n",
    "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
    "\n",
    "test_log_reg_fit(LogisticRegression)"
   ],
   "outputs": [],
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T08:35:32.462336Z",
     "start_time": "2025-04-23T08:35:32.457917Z"
    }
   },
   "source": [
    "def test_log_reg_predict_proba(StudentLogisticRegression):\n",
    "\n",
    "    student_lr_clf = StudentLogisticRegression(max_iter=5)\n",
    "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
    "                            [-0.52817175, -1.07296862],\n",
    "                            [ 0.86540763, -2.3015387 ],\n",
    "                            [ 1.74481176, -0.7612069 ],\n",
    "                            [ 0.3190391,  -0.24937038]])\n",
    "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
    "    student_lr_clf.fit(test_case_X, test_case_y)\n",
    "    student_ans = student_lr_clf.predict_proba(test_case_X)\n",
    "    required_ans = np.array([0.49052814, 0.5029122 , 0.48449386, 0.48864172, 0.50241207])\n",
    "\n",
    "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
    "\n",
    "test_log_reg_predict_proba(LogisticRegression)"
   ],
   "outputs": [],
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T08:35:33.243221Z",
     "start_time": "2025-04-23T08:35:33.239205Z"
    }
   },
   "source": [
    "def test_log_reg_predict(StudentLogisticRegression):\n",
    "\n",
    "    student_lr_clf = StudentLogisticRegression(max_iter=5)\n",
    "    np.random.seed(1)\n",
    "    test_case_X = np.random.randn(50, 2)\n",
    "    test_case_y = np.random.randint(0, 2, 50)\n",
    "    student_lr_clf.fit(test_case_X, test_case_y)\n",
    "    student_ans = student_lr_clf.predict(test_case_X)\n",
    "    required_ans = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
    "                             0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1])\n",
    "\n",
    "    assert np.mean(np.abs(student_ans - required_ans)) <= 0.02\n",
    "\n",
    "test_log_reg_predict(LogisticRegression)"
   ],
   "outputs": [],
   "execution_count": 51
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Measures of Impurity and their Reduction [15 pts]**\n",
    "To grow a classification tree, instead of a binary error (1/0), measures of impurity are used to see how good a leaf node is. Recall that we discussed about entropy being one such measure of impurity. We will be working with entropy and comparing it to another metric called the gini index."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.1. Measures of Impurity [9 pts]**\n",
    "\n",
    "For this problem, consider that you have a binary classification problem of two classes, the positive class $1$ and the negative class $0$.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.1.1. Entropy [2 pts]**\n",
    "\n",
    "Please complete the entropy function."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T08:38:59.105894Z",
     "start_time": "2025-04-23T08:38:59.103446Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def cross_entropy(prob_class1):\n",
    "\n",
    "    \"\"\"\n",
    "    Returns the cross-entropy value of a node given the probability of a sample belonging to class 1 in the node.\n",
    "\n",
    "    Args:\n",
    "        prob_class1: The probability of a sample belonging to class 1 in a decision tree node\n",
    "\n",
    "    Returns:\n",
    "        ce: The cross-entropy value for the node\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO START: Complete the function\n",
    "    return -np.nansum(prob_class1 * np.log2(prob_class1) + (1 - prob_class1) * np.log2(1 - prob_class1))\n",
    "    # TODO END\n",
    "\n",
    "assert cross_entropy(0.5) == 1"
   ],
   "outputs": [],
   "execution_count": 54
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.1.2. Gini Index [2 pts]**\n",
    "\n",
    "Gini index is another measure of impurity. For an K-class classification problem, gini index is calculated as follows.\n",
    "\n",
    "$$\\text{Gini Index} = \\sum_{k = 1}^{K} p_k(1 - p_k)$$\n",
    "\n",
    "Complete the following function for calculating the gini index of a binary-class problem (k = 2)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T08:50:51.241014Z",
     "start_time": "2025-04-23T08:50:51.238773Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def gini_index(prob_class1) -> float:\n",
    "\n",
    "    \"\"\"\n",
    "    Returns the gini-index value of a node given the probability of a sample belonging to class 1 in the node.\n",
    "\n",
    "    Args:\n",
    "        prob_class1: The probability of a sample belonging to class 1 in a decision tree node\n",
    "\n",
    "    Returns:\n",
    "        gi: The gini-index value for the node\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO START: Complete the function\n",
    "    return np.nansum(prob_class1 * (1 - prob_class1)) * 2\n",
    "    # TODO END\n",
    "\n",
    "assert gini_index(0.5) == 0.5"
   ],
   "outputs": [],
   "execution_count": 58
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.1.2. Plot [5 pts, manually graded]**\n",
    "\n",
    "Please complete the impurity_measures_plot function and generate a plot of the entropy and gini index values with respect to the class 1 probability values. Both the impurity measures should be on the same plot.\n",
    "\n",
    "- Submit the generated plot along with the written homework solutions.\n",
    "- Make sure the plot has a title, legend and axes labels.\n",
    "- Comment on why cross entropy and gini index are suitable measures of impurity based on the plot."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T09:20:31.910059Z",
     "start_time": "2025-04-23T09:20:31.852671Z"
    }
   },
   "source": [
    "def impurity_measures_plot():\n",
    "\n",
    "    '''\n",
    "    Plots the cross entropy and gini index values with respect to the probability values of class 1.\n",
    "\n",
    "    Args:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    Notes:\n",
    "        1. Please do not change the provided code\n",
    "        2. Both cross entropy and gini index should be on the same scatter plot\n",
    "    '''\n",
    "\n",
    "    prob_class1_arr = np.arange(1, 1000)/1000\n",
    "    ce_arr = np.array([cross_entropy(p) for p in prob_class1_arr])\n",
    "    gi_arr = np.array([gini_index(p) for p in prob_class1_arr])\n",
    "\n",
    "    # TODO START: Complete the function\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot(prob_class1_arr, ce_arr, label=\"CROSS ENTROPHY\", linewidth=2)\n",
    "    plt.plot(prob_class1_arr, gi_arr, label=\"GINI INDEX\",   linewidth=2)\n",
    "    plt.title(\"impurity_measures_plot\")\n",
    "    plt.xlabel(\"P(class = 1)\")\n",
    "    plt.ylabel(\"Impurity\")\n",
    "    plt.legend()\n",
    "    plt.style.use(\"grayscale\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # TODO END\n",
    "\n",
    "impurity_measures_plot()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 70
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.2. Reduction in Impurity [6 pts]**\n",
    "\n",
    "Recall that we also discussed information gain which is the change in entropy from the parent node to the children nodes. Gini reduction is similar to information gain except you replace entropy values with gini index."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.2.1. Information Gain [3 pts]**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T10:03:12.002693Z",
     "start_time": "2025-04-23T10:03:11.999719Z"
    }
   },
   "source": [
    "def information_gain(num_samples_parent, num_class1_parent, num_samples_child1, num_class1_child1):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        num_samples_parent: Number of samples in the parent node\n",
    "        num_class1_parent: Number of samples of class 1 in parent node\n",
    "        num_samples_child1: Number of samples in the first child node\n",
    "        num_class1_child1: Number of samples of class 1 in the first child node\n",
    "\n",
    "    Returns:\n",
    "        ig: Information Gain\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO START: Complete the function\n",
    "    # 1. You will need to calculate cross-entropy for the parent and child nodes\n",
    "    # 2. Use the above entropies to finally calculate information gain\n",
    "\n",
    "    # H(Y)\n",
    "    p_parent = num_class1_parent / num_samples_parent\n",
    "    H_parent = cross_entropy(p_parent)\n",
    "\n",
    "    # H(Y = 1|X)\n",
    "    n1 = num_samples_child1\n",
    "    p1 = num_class1_child1 / n1\n",
    "    H1 = cross_entropy(p1)\n",
    "\n",
    "    # H(Y = 0|X)\n",
    "    n2 = num_samples_parent - n1\n",
    "    num_class1_child2 = num_class1_parent - num_class1_child1\n",
    "    p2 = num_class1_child2 / n2\n",
    "    H2 = cross_entropy(p2)\n",
    "\n",
    "    # 4) 정보 이득 = H(Y) - H(Y|X) = H(Y) - (P(Y=1)H(Y=1|X) + P(Y=0)H(Y=0|X))\n",
    "    ig = H_parent - ((n1 / num_samples_parent) * H1 + (n2 / num_samples_parent) * H2)\n",
    "    return ig\n",
    "\n",
    "    # TODO END\n",
    "\n",
    "\n",
    "assert np.abs(information_gain(100, 60, 30, 5) - 0.251) < 0.01"
   ],
   "outputs": [],
   "execution_count": 74
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.2.2. Gini Reduction [3 pts]**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T10:08:35.339664Z",
     "start_time": "2025-04-23T10:08:35.336385Z"
    }
   },
   "source": [
    "def gini_reduction(num_samples_parent, num_class1_parent, num_samples_child1, num_class1_child1):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        num_samples_parent: Number of samples in the parent node\n",
    "        num_class1_parent: Number of samples of class 1 in parent node\n",
    "        num_samples_child1: Number of samples in the first child node\n",
    "        num_class1_child1: Number of samples of class 1 in the first child node\n",
    "\n",
    "    Returns:\n",
    "        gr: Gini Reduction\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO START: Complete the function\n",
    "\n",
    "    # H(Y)\n",
    "    p_parent  = num_class1_parent / num_samples_parent\n",
    "    H_parent  = gini_index(p_parent)\n",
    "\n",
    "    # H(Y = 1|X)\n",
    "    p_child1 = num_class1_child1 / num_samples_child1\n",
    "    H1 = gini_index(p_child1)\n",
    "\n",
    "    # H(Y = 0|X)\n",
    "    num2  = num_samples_parent - num_samples_child1\n",
    "    num_class1_child2 = num_class1_parent - num_class1_child1\n",
    "    p_child2 = num_class1_child2 / num2\n",
    "    H2 = gini_index(p_child2)\n",
    "\n",
    "    # 지니 감소?\n",
    "    gr = H_parent - ((num_samples_child1 / num_samples_parent) * H1 + (num2 / num_samples_parent) * H2)\n",
    "    return gr\n",
    "\n",
    "    # TODO END\n",
    "\n",
    "assert np.abs(gini_reduction(100, 60, 30, 5) - 0.161) < 0.01"
   ],
   "outputs": [],
   "execution_count": 76
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6. Decision Tree [35 pts]**\n",
    "\n",
    "In this section you will be training a decision tree classifier to predict the presence of diabetes in a person given various input features. The diabetes dataset that we are using is from the [2013-2014  National Health and Nutrition Examination Survey (NHANES)](https://wwwn.cdc.gov/nchs/nhanes/Default.aspx). We have reduced the dataset to only 20 features but the original dataset had over 1,800 features. `CSE-SSU_hw1_diabetes_train.csv` and `CSE-SSU_hw1_diabetes_X_test.csv` are the datasets that you would be using for training and testing respectively."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.1. Load Datasets**\n",
    "\n",
    "Read the files `CSE-SSU_hw1_diabetes_train.csv` and `CSE-SSU_hw1_diabetes_X_test.csv` into train_df and test_df respectively in the `load_diabetes_datasets` function."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T10:08:50.297024Z",
     "start_time": "2025-04-23T10:08:50.294056Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_diabetes_datasets():\n",
    "    '''\n",
    "    Args:\n",
    "        None\n",
    "    Returns:\n",
    "        train_df, test_df\n",
    "    '''\n",
    "\n",
    "    # TODO START: Complete the function\n",
    "\n",
    "    train_df = pd.read_csv('CSE-SSU_hw1_diabetes_train.csv')\n",
    "    test_df = pd.read_csv('CSE-SSU_hw1_diabetes_X_test.csv')\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "    # TODO END"
   ],
   "outputs": [],
   "execution_count": 77
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.2. Preprocess Datasets [10 pts]**\n",
    "\n",
    "The datasets we have provided are not ready-to-use for machine learning and requires preprocessing. We want you to perform feature selection and handle missing values in both the training and test datasets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.2.1. Feature Selection**\n",
    "\n",
    "For feature selection, you should retain the following features at least and experiment including/excluding the remaining features.\n",
    "\n",
    "- 'RIDAGEYR'\n",
    "- 'BMXWAIST'\n",
    "- 'BMXHT'\n",
    "- 'LBXTC'\n",
    "- 'BMXLEG'\n",
    "- 'BMXWT'\n",
    "- 'BMXBMI'\n",
    "- 'RIDRETH1'\n",
    "- 'BPQ020'\n",
    "- 'ALQ120Q'\n",
    "- 'DMDEDUC2'\n",
    "- 'RIAGENDR'\n",
    "- 'INDFMPIR'\n",
    "\n",
    "The column `DIABETIC` in the training dataset is the target variable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.2.2. Handling Missing Values**\n",
    "\n",
    "We recommend you to drop rows with missing values in the training set. However, you should not drop rows with missing values in the test set. Instead, you should impute missing values in the test set with the mean of the corresponding columns in the training set."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T15:08:54.440924Z",
     "start_time": "2025-04-18T15:08:54.427277Z"
    }
   },
   "source": [
    "train_df, test_df = load_diabetes_datasets()\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess_datasets(train_df, test_df):\n",
    "    '''\n",
    "    Args:\n",
    "        train_df\n",
    "        test_df\n",
    "    Returns:\n",
    "        train_df (preprocessed)\n",
    "        test_df (preprocessed)\n",
    "    Note:\n",
    "        1. At least the following columns should be present in the final train_df:\n",
    "            - 'RIDAGEYR'\n",
    "            - 'BMXWAIST'\n",
    "            - 'BMXHT'\n",
    "            - 'LBXTC'\n",
    "            - 'BMXLEG'\n",
    "            - 'BMXWT'\n",
    "            - 'BMXBMI'\n",
    "            - 'RIDRETH1'\n",
    "            - 'BPQ020'\n",
    "            - 'ALQ120Q'\n",
    "            - 'DMDEDUC2'\n",
    "            - 'RIAGENDR'\n",
    "            - 'INDFMPIR'\n",
    "            - 'DIABETIC'\n",
    "        2. test_df will have all the columns in train_df except the 'DIABETIC' column\n",
    "        3. Drop any rows in train_df that have missing values\n",
    "        4. DO NOT drop rows with missing values test_df. Impute missing values in test_df with the means of the corresponding columns in train_df.\n",
    "    '''\n",
    "\n",
    "    # TODO START: Complete the function\n",
    "\n",
    "    # TODO END\n",
    "\n",
    "train_df_processed, test_df_processed = preprocess_datasets(train_df, test_df)\n"
   ],
   "outputs": [],
   "execution_count": 91
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.3. Decision Tree Training with Pruning [14 pts]**\n",
    "\n",
    "Next, we will be fitting a decision tree classifier and prune the tree appropriately. The `DecisionTreeClassifier` in scikit-learn uses a way of pruning called **Minimal Cost-Complexity Pruning**. We won't cover the specifics, but you can learn more from this [link](https://scikit-learn.org/stable/modules/tree.html#minimal-cost-complexity-pruning) if you wish. But, you don't need to learn the details in order to use it effectively. The amount of pruning is entirely dependent on the value of the `ccp_alpha` parameter. In order to tune the `ccp_alpha` parameter, you will use [cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html). The purpose of cross-validation is to estimate how well a model will generalize on unseen data.\n",
    "\n",
    "Implement the function `best_ccp_alpha_f1` to do automatic tuning of the `ccp_alpha` parameter.  Your function should vary the value of the `ccp_alpha` parameter and return the value for `ccp_alpha` with the highest cross-validation F1 score over the given dataset `train_df`. The sklearn library has a [built-in function](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.cost_complexity_pruning_path) to generate a list of effective ccp_alphas. Given the imbalanced nature of the dataset, most of the people in the data set are non-diabetic. You can get a model with very high test accuracy by always predicting no one is diabetic. To address this problem, more importance should be given to the [F1 score](https://en.wikipedia.org/wiki/F-score) of your model rather than the classification accuracy.\n",
    "\n",
    "For this problem, you need to have at least 80% accuracy and a F1 score of 0.2 on the test dataset to get full points."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T15:09:04.562512Z",
     "start_time": "2025-04-18T15:08:54.454989Z"
    }
   },
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "train_df, test_df = preprocess_datasets(train_df, test_df)\n",
    "\n",
    "def best_ccp_alpha_f1(train_df):\n",
    "    \"\"\"\n",
    "    Returns the pruning parameter (best_ccp_alpha) with the highest cross-validation F1 score along with the\n",
    "    five cross-validation F1 scores corresponding (cv_f1_scores).\n",
    "\n",
    "    Args:\n",
    "        train_df\n",
    "\n",
    "    Returns:\n",
    "        best_ccp_alpha: the tuned best ccp alpha value\n",
    "        cv_f1_scores: the five cross-validation F1 scores\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO START: Complete the function\n",
    "\n",
    "    # TODO END\n",
    "\n",
    "def refit_and_predict(train_df, test_df, best_ccp_alpha):\n",
    "    \"\"\"\n",
    "    Fit a decision tree classifier on the training data using the best_ccp_alpha value and output the predictions on the\n",
    "    test set.\n",
    "\n",
    "    Args:\n",
    "        train_df\n",
    "        test_df\n",
    "        best_ccp_alpha\n",
    "\n",
    "    Returns:\n",
    "        y_test_pred: The predicted values for the test set\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO START: Complete the function\n",
    "\n",
    "    # TODO END\n",
    "\n",
    "best_ccp_alpha, cv_f1_scores = best_ccp_alpha_f1(train_df)\n",
    "y_test_pred_DT = refit_and_predict(train_df, test_df, best_ccp_alpha)"
   ],
   "outputs": [],
   "execution_count": 92
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.4. Computing Confidence Intervals [5 pts]**\n",
    "\n",
    "Even though you may have computed the average F1 score across the held-out folds during cross validation, how confident can you be that the number you computed is the true F1 score for that set of features? If you try rerunning your code with a different random seed, you may actually get a different F1 score. But which one is right?\n",
    "\n",
    "In order to answer this question, we will compute a confidence interval based on the Student's t-distribution, which will tell us with 99\\% confidence that the true mean is within a lower and upper bound. To compute the confidence interval, we need to compute the sample mean, $\\bar{x}$, sample standard deviation, $S$, and the number of observations for each classifier, $n$. ***In our specific case, the number of observations should be 5 because we have 5 reported F1 scores from cross-validation.***\n",
    "\n",
    "Then, the confidence interval is computed by\n",
    "    \n",
    "$$\\bar{x} \\pm t \\cdot \\frac{S}{\\sqrt{n}}$$\n",
    "\n",
    "Here, $t$ is the critical value, which we can look up using the provided t-table (https://www.stat.colostate.edu/inmem/gumina/st201/pdf/Utts-Heckard_t-Table.pdf). (Round up the critical value to the second digit below the decimal point) For example, when $n=10$, if we are looking for a 99\\% confidence interval, then the number in the 99\\% confidence column with degrees of freedom of $n-1=9$ would be $t=3.25$. Then, we can plug in all of the statistics into the confidence interval formula and get a range of values for which we are 99\\% confident that the true F1 score of the classifier falls between.\n",
    "\n",
    "For this computation, we should use the unbiased estimator of the variance, which means that the degrees of freedom on the standard deviation calculation must be set. Look in the optional arguments of np.std to learn more."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T15:09:04.575691Z",
     "start_time": "2025-04-18T15:09:04.572493Z"
    }
   },
   "source": [
    "from math import sqrt\n",
    "# 사전 방식(t‑table 일부; 두 번째 소수점 자리 올림 값)\n",
    "_t_lookup = {\n",
    "    # df : (t_(0.80/2), t_(0.99/2))   0.80 → α=0.20, 0.99 → α=0.01\n",
    "    4:  (1.53, 4.60),\n",
    "    5:  (1.48, 4.03),\n",
    "    6:  (1.44, 3.71),\n",
    "    7:  (1.42, 3.50),\n",
    "    8:  (1.40, 3.36),\n",
    "    9:  (1.38, 3.25),\n",
    "    10: (1.37, 3.17),\n",
    "    20: (1.33, 2.85),\n",
    "    30: (1.31, 2.75),\n",
    "}\n",
    "\n",
    "def _t_critical(df, confidence):\n",
    "    \"\"\"\n",
    "    df : 자유도, confidence : 0.99 또는 0.80\n",
    "    SciPy 가 있으면 정확히, 없으면 lookup 사용\n",
    "    \"\"\"\n",
    "    lookup = {\n",
    "        (4, 0.99): 4.60,\n",
    "        (4, 0.80): 1.53,\n",
    "        # 필요한 경우 df 키를 추가\n",
    "    }\n",
    "    if (df, confidence) in lookup:\n",
    "        return lookup[(df, confidence)]\n",
    "    raise ValueError(\"t‑값 테이블에 없는 자유도 / 신뢰수준입니다.\")\n",
    "\n",
    "def calculate_confidence_interval(cv_f1_scores):\n",
    "    '''\n",
    "    Args:\n",
    "        cv_f1_scores      :   np.array, reported cross-validation F1 scores\n",
    "    Returns:\n",
    "        interval    :   np.array, lower bound and upper bound of the 99% confidence interval\n",
    "    '''\n",
    "\n",
    "    # TODO START: Complete the function\n",
    "    # This function should be able to handle two confidence levels of 99% and 80%\n",
    "\n",
    "    # TODO END"
   ],
   "outputs": [],
   "execution_count": 93
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T10:10:58.576188Z",
     "start_time": "2025-04-23T10:10:58.558836Z"
    }
   },
   "source": [
    "def test_confidence_intervals():\n",
    "    data = np.array([15.6, 16.2, 22.5, 20.5, 16.4])\n",
    "    result = np.round(calculate_confidence_interval(data), 3)\n",
    "    interval = np.array([11.918, 24.562])\n",
    "    assert (np.array_equal(interval, result))\n",
    "\n",
    "test_confidence_intervals()"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'calculate_confidence_interval' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[78], line 7\u001B[0m\n\u001B[1;32m      4\u001B[0m     interval \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([\u001B[38;5;241m11.918\u001B[39m, \u001B[38;5;241m24.562\u001B[39m])\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m (np\u001B[38;5;241m.\u001B[39marray_equal(interval, result))\n\u001B[0;32m----> 7\u001B[0m \u001B[43mtest_confidence_intervals\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[78], line 3\u001B[0m, in \u001B[0;36mtest_confidence_intervals\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mtest_confidence_intervals\u001B[39m():\n\u001B[1;32m      2\u001B[0m     data \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([\u001B[38;5;241m15.6\u001B[39m, \u001B[38;5;241m16.2\u001B[39m, \u001B[38;5;241m22.5\u001B[39m, \u001B[38;5;241m20.5\u001B[39m, \u001B[38;5;241m16.4\u001B[39m])\n\u001B[0;32m----> 3\u001B[0m     result \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mround(\u001B[43mcalculate_confidence_interval\u001B[49m(data), \u001B[38;5;241m3\u001B[39m)\n\u001B[1;32m      4\u001B[0m     interval \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([\u001B[38;5;241m11.918\u001B[39m, \u001B[38;5;241m24.562\u001B[39m])\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m (np\u001B[38;5;241m.\u001B[39marray_equal(interval, result))\n",
      "\u001B[0;31mNameError\u001B[0m: name 'calculate_confidence_interval' is not defined"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6.5. Performance Table [6 pts, manually graded]**\n",
    "\n",
    "Repeat the process for two other sets of features and present a performance table (like the one shown below) that compares the F1 scores and confidence intervals of the three sets of features, indicating which one is your chosen best set. Remember that each set should include the 13 features mentioned earlier. As mentioned earlier, submit this table along with the written homework solutions as this is manually graded.\n",
    "\n",
    "---\n",
    "\n",
    "S.No. | Features | Best CCP Alpha | Mean Cross-validation F1 Score | Cross-validation F1 Score Confidence Interval\n",
    "--- | --- | --- | --- | ---\n",
    "1 | Set 1 | | |\n",
    "2 | Set 2 | | |\n",
    "3 | Set 3 | | |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **7 Fit the Logistic Regression on Diabetes dataset (With only the required features in 6.2) [14 pts]**\n",
    "\n",
    "Fit a simple logistic regression on the training data using l2 penalty, $\\alpha$ = 0.01, maximum of iterations = 1000, and weight for the regularization consant for the l2 penalty  term is 0.001.\n",
    "You should be rescaling features using MinMaxScaler from sklearn.preprocessing to make sure that the features are properly scaled for learning."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T10:11:12.057875Z",
     "start_time": "2025-04-23T10:11:12.026101Z"
    }
   },
   "source": [
    "train_df, test_df = load_diabetes_datasets()\n",
    "train_df, test_df = preprocess_datasets(train_df, test_df)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[79], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m train_df, test_df \u001B[38;5;241m=\u001B[39m load_diabetes_datasets()\n\u001B[0;32m----> 2\u001B[0m train_df, test_df \u001B[38;5;241m=\u001B[39m \u001B[43mpreprocess_datasets\u001B[49m(train_df, test_df)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'preprocess_datasets' is not defined"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T15:09:04.801280Z",
     "start_time": "2025-04-18T15:09:04.617323Z"
    }
   },
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def fit_and_predict_logistic(train_df, test_df):\n",
    "    \"\"\"\n",
    "    Fit a logistic regression classifier on the training data and output the predictions on the\n",
    "    test set.\n",
    "\n",
    "    Args:\n",
    "        train_df\n",
    "        test_df\n",
    "\n",
    "    Returns:\n",
    "        y_test_pred: The predicted values for the test set\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO START: Complete the function\n",
    "\n",
    "    # TODO END\n",
    "\n",
    "y_test_pred_logistic = fit_and_predict_logistic(train_df, test_df)\n",
    "\n",
    "print(y_test_pred_logistic)\n",
    "np.unique(y_test_pred_logistic, return_counts=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wk/2jprnydj1g57l2bjlg47zytm0000gn/T/ipykernel_6739/2634416923.py:137: RuntimeWarning: divide by zero encountered in matmul\n",
      "  grad = X.T @ (p_hat - y)\n",
      "/var/folders/wk/2jprnydj1g57l2bjlg47zytm0000gn/T/ipykernel_6739/2634416923.py:137: RuntimeWarning: overflow encountered in matmul\n",
      "  grad = X.T @ (p_hat - y)\n",
      "/var/folders/wk/2jprnydj1g57l2bjlg47zytm0000gn/T/ipykernel_6739/2634416923.py:137: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad = X.T @ (p_hat - y)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([1813,  222]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 96
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8 Best Model on Diabetes Dataset, and Why. [6pts, manually graded]**\n",
    "\n",
    "Which one of Logistic Regression, and Decision Tree will provide the best performance on this dataset. Give reasons for your answer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "x8wDM7Y9hkLg"
   },
   "source": [
    "## Submit to LMS\n",
    "Congratulation on completing HW1 coding assignment! The last step you need to do is to save this iPython notebook, rename it as **hw1-{yourstudentID}.ipynb** and submit it to LMS.\n",
    "\n",
    "*If using Google Colab, in order to download the notebook from Google Colab, click File->Download->Download .ipynb*\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
