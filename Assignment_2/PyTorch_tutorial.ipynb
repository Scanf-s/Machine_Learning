{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwxBJg3054tF"
      },
      "source": [
        "# PyTorch Tutorial\n",
        "\n",
        "This tutorial is mostly based on:\n",
        "\n",
        "* https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
        "* https://pytorch.org/tutorials/beginner/pytorch_with_examples.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4GoE0Hq54tG",
        "outputId": "d08a3e6f-ad16-443d-fc52-3574d1115176"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0+cu124\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQJXKDxjfJG7"
      },
      "source": [
        "## What is PyTorch?\n",
        "PyTorch is a framework for working with deep neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nCxSbP1eSqq"
      },
      "source": [
        "## Why use PyTorch?\n",
        "- Autograd functionality computes gradients automatically\n",
        "- PyTorch integrates well with the Python data science stack, including NumPy, SciPy and Pandas for efficient data loading and processing pipelines\n",
        "- GPU acceleration allows for fast training and inference, utilizing the parallel processing power of GPUs\n",
        "- A large library of useful deep learning functions and modules are already built-in, enabling faster development and deployment of models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig65bPGsDeqq"
      },
      "source": [
        "## Overview\n",
        "- `torch.Tensor` basic tensor operation\n",
        "- `torch.Tensor.grad` auto-differentiation\n",
        "- `torch.cuda` devices other than CPU\n",
        "- `torch.nn` neural network blocks\n",
        "- `torch.utils.data` dataset and dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5AXjLCz54tJ"
      },
      "source": [
        "## PyTorch Tensors\n",
        "\n",
        "PyTorch tensors are just like NumPy arrays, and they include many of the same operations you are used to from NumPy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KSKVAxA54tJ"
      },
      "source": [
        "Construct a tensor of size $5 \\times 3$ with random values:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EKqc9Rq54tK",
        "outputId": "57143f21-ca77-457d-8b28-94ce34cabb0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3743, 0.2606, 0.3921],\n",
            "        [0.3763, 0.7383, 0.4317],\n",
            "        [0.3076, 0.1430, 0.0710],\n",
            "        [0.1086, 0.7765, 0.0204],\n",
            "        [0.5524, 0.7195, 0.0082]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.rand(5, 3)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ9KbuE554tM"
      },
      "source": [
        "Construct a matrix filled with zeros and of dtype int:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTSHE-nx1Nd9",
        "outputId": "19a016e5-4ad0-4c81-c90a-84ac5ab8149c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 0, 0],\n",
            "        [0, 0, 0],\n",
            "        [0, 0, 0],\n",
            "        [0, 0, 0],\n",
            "        [0, 0, 0]], dtype=torch.int32)\n"
          ]
        }
      ],
      "source": [
        "# Create a 5x3 tensor filled with zeros using int32 datatype\n",
        "# torch.int32 provides a balance between memory usage and range\n",
        "x = torch.zeros(5, 3, dtype=torch.int32)  # torch.int = torch.int32 = 32-bit signed integer, range of (-2^31) to (2^31 - 1)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCB3e8Va1Nd9"
      },
      "source": [
        "Construct a matrix filled with zeros and of dtype long:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oa8OceHw54tN",
        "outputId": "fdb151ec-8a0a-421e-e8a8-fa4e0a97c05e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 0, 0],\n",
            "        [0, 0, 0],\n",
            "        [0, 0, 0],\n",
            "        [0, 0, 0],\n",
            "        [0, 0, 0]])\n"
          ]
        }
      ],
      "source": [
        "# Create a 5x3 tensor filled with zeros using int64 (long) datatype\n",
        "# Use when working with large integers or when indexing large tensors\n",
        "x = torch.zeros(5, 3, dtype=torch.long)  # torch.long = torch.int64 = 64-bit signed integer, range of (-2^63) to (2^63 - 1)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiiA773h54tP"
      },
      "source": [
        "Make a tensor from a list of values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5fCpQsy54tP",
        "outputId": "47e3ea1c-fc02-420b-e45a-a6cd753b9d9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3])\n",
            "torch.int64\n",
            "tensor([1., 2., 3.])\n",
            "torch.float32\n"
          ]
        }
      ],
      "source": [
        "# PyTorch automatically infers appropriate datatypes based on input values\n",
        "# Integer without decimal points create tensors with torch.int64 (long) datatype\n",
        "x = torch.tensor([1, 2, 3])\n",
        "print(x)\n",
        "print(x.dtype)\n",
        "\n",
        "# Floating point literals with decimal points create tensors with torch.float32 datatype\n",
        "# float32 is the default floating-point precision in PyTorch\n",
        "x = torch.tensor([1., 2., 3.])\n",
        "print(x)\n",
        "print(x.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNNJG12d54tS"
      },
      "source": [
        "Create a tensor based on another tensor (inherit size and dtype, unless otherwise specified):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95vGdEesf-8I"
      },
      "source": [
        "By default, the returned Tensor of `new_ones` has the same torch.dtype and torch.device as input tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "At5-rz6O54tS",
        "outputId": "d0edd4f5-981a-4782-fa37-3fb30788fde2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "tensor([[-1.6698e+00, -4.0995e-02,  6.1336e-01],\n",
            "        [-7.5922e-01, -7.8970e-01, -1.4647e-01],\n",
            "        [ 1.6595e-01,  2.4633e-01, -4.6894e-01],\n",
            "        [ 9.4158e-01, -1.0496e+00, -4.7921e-01],\n",
            "        [-1.0183e+00,  1.3282e-03, -1.6889e-01]])\n"
          ]
        }
      ],
      "source": [
        "# Create a new tensor with the same dtype and device as x, but filled with ones\n",
        "# new_* methods (like new_ones) inherit properties (dtype and device) from the source tensor\n",
        "# new_* methods take output matrix size as input parameters\n",
        "x = x.new_ones(5, 3)\n",
        "print(x)\n",
        "\n",
        "# Create a tensor with same shape as x but filled with random values from normal distribution\n",
        "# Note: randn samples from standard normal distribution (mean=0, std=1)\n",
        "x = torch.randn_like(x, dtype=torch.float)  # override dtype!\n",
        "print(x)                                    # result has the same size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnbhwMxT54tW"
      },
      "source": [
        "Get the size object of a tensor, an object which supports tuple operations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WN8pUnL54tW",
        "outputId": "b9224270-4f29-4895-bfb0-133edc4ff23d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 3])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# size() is a method that returns a torch.size object\n",
        "# It shows the dimensions of the tensor in each axis\n",
        "print(x.size())\n",
        "\n",
        "# shape is an attribute that provides the same information as size()\n",
        "# This is equivalent to size() but follows NumPy's style convention\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Yl7e9aq54tY"
      },
      "source": [
        "Operations on tensors use similar syntax to NumPy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88YT3jWB54tZ",
        "outputId": "390d8ba4-f78b-4154-b51c-0ba152c28002"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x + y: tensor([[2., 2., 2.],\n",
            "        [2., 2., 2.],\n",
            "        [2., 2., 2.],\n",
            "        [2., 2., 2.],\n",
            "        [2., 2., 2.]])\n",
            "torch.add(x, y): tensor([[2., 2., 2.],\n",
            "        [2., 2., 2.],\n",
            "        [2., 2., 2.],\n",
            "        [2., 2., 2.],\n",
            "        [2., 2., 2.]])\n",
            "x: tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "y: tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "# Create two 5x3 tensors filled with ones\n",
        "x = torch.ones(5, 3)\n",
        "y = torch.ones(5, 3)\n",
        "\n",
        "# Element-wise addition using operator syntax (creates a new tensor)\n",
        "print(\"x + y:\", x + y)\n",
        "\n",
        "# Element-wise addition using function syntax (equivalent to operator syntax)\n",
        "print(\"torch.add(x, y):\", torch.add(x, y))\n",
        "\n",
        "# Original tensors remain unchanged\n",
        "print(\"x:\", x)\n",
        "print(\"y:\", y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA5GHiwL54tb"
      },
      "source": [
        "PyTorch also supports in-place operations (method names end in '_'):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIVoVr4t54tb",
        "outputId": "f497829e-4b5a-4825-8984-ed650eeac445"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 2., 2.],\n",
            "        [2., 2., 2.],\n",
            "        [2., 2., 2.],\n",
            "        [2., 2., 2.],\n",
            "        [2., 2., 2.]])\n"
          ]
        }
      ],
      "source": [
        "# In-place operations modify the tensor directly and are denoted by a trailing underscore (_)\n",
        "# add_ adds x to y and stores the result in y (equivalent to y = y + x)\n",
        "y.add_(x)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7SsQGJZ1Nd-"
      },
      "source": [
        "#### Qn: What happens if you run y.add_(x) again?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyI2nSIv1Nd-",
        "outputId": "a955fc5a-8383-4795-a94a-731ae9d43db6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[3., 3., 3.],\n",
            "        [3., 3., 3.],\n",
            "        [3., 3., 3.],\n",
            "        [3., 3., 3.],\n",
            "        [3., 3., 3.]])\n"
          ]
        }
      ],
      "source": [
        "# What happens if you run y.add_(x) again?\n",
        "# y.add_(x) will add x to y again, but this time it will use the updated y value\n",
        "# This is because in-place operations modify the tensor in place\n",
        "# So if you run y.add_(x) again, it will use the new value of y after the first addition\n",
        "y.add_(x)\n",
        "print(y) # 메모리 참조를 통해 Add를 수행했으므로 3으로 채워진다\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaRuJ9eS1Nd-"
      },
      "source": [
        "#### Qn: What function does \"Run before\" command serve in Google colab?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1Fc2EBVq1Nd-"
      },
      "outputs": [],
      "source": [
        "# NOTE: For jupyter notebooks, do not run cells that update values of certain variables,\n",
        "# tensors or have inplace operations multiple times!\n",
        "# This is because the variable values will be updated and the next cells that use those variables will give incorrect results.\n",
        "\n",
        "# Solution for Google colab:\n",
        "# Step 1: Edit >> Clear all outputs without restarting the kernel (수정 -> 모든 출력 제거)\n",
        "# Step 2: Runtime >> Interrupt execution (this will interrupt the execution of the current cell) (런타임 -> 실행 중단)\n",
        "# Step 3: Runtime >> Restart session (this will restart the kernel and clear all variables) (런타임 -> 세션 다시 시작)\n",
        "# Step 4: Runtime >> Run before (this will run all the cells before the current cell) (런타임 -> 이전 셀 실행)\n",
        "\n",
        "# These steps ensure that the variable values are not changed unexpectedly!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pB4lLJtW5FbL"
      },
      "source": [
        "Example of broadcasting:\n",
        "\n",
        "To learn more:\n",
        "- Broadcasting basics using Numpy: https://numpy.org/doc/stable/user/basics.broadcasting.html\n",
        "- Visual Example of Broadcasting using Numpy:  https://numpy.org/doc/stable/_images/broadcasting_2.png\n",
        "- <a href=\"https://numpy.org/doc/stable/user/basics.broadcasting.html\">\n",
        "  <img src=\"https://numpy.org/doc/stable/_images/broadcasting_2.png\"\n",
        "       alt=\"Broadcasting example\" width=\"500\"/>\n",
        "</a>\n",
        "- Broadcasting basics using PyTorch: https://pytorch.org/docs/stable/notes/broadcasting.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhKtv0wv1Nd-",
        "outputId": "e76d4aa4-619a-491a-a47d-194b2700e710"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of a: (4, 3)\n",
            "Shape of b: (3,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.,  2.,  3.],\n",
              "       [11., 12., 13.],\n",
              "       [21., 22., 23.],\n",
              "       [31., 32., 33.]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import numpy as np\n",
        "a = np.array([[ 0.0,  0.0,  0.0],\n",
        "              [10.0, 10.0, 10.0],\n",
        "              [20.0, 20.0, 20.0],\n",
        "              [30.0, 30.0, 30.0]])\n",
        "b = np.array([1.0, 2.0, 3.0]) # 차원을 a 기준으로 맞춰서 값 broadcasting\n",
        "\n",
        "print(f\"Shape of a: {a.shape}\")\n",
        "print(f\"Shape of b: {b.shape}\")\n",
        "a + b\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYELL_yO1Nd_"
      },
      "source": [
        "#### Qn: What happens if a has shape (4,3) and b has shape (4,)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hdSb1FXs1Nd_"
      },
      "outputs": [],
      "source": [
        "# b = np.array([1.0, 2.0, 3.0, 4.0])\n",
        "# a + b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJ96VUUj5LGZ",
        "outputId": "25c65a7d-3f7c-4b55-fa64-030b3d33741e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x + y: [[4. 4. 4.]\n",
            " [4. 4. 4.]\n",
            " [4. 4. 4.]\n",
            " [4. 4. 4.]\n",
            " [4. 4. 4.]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# NumPy broadcasting: scalar to array\n",
        "# The scalar value 3 is broadcast to match the (shape of x) = (5,3)\n",
        "x = np.ones((5,3)) # 1로 채워진 5행 3열 np.array\n",
        "y = 3 # x를 기준으로 y가 3으로 채워진 5행 3열 np.array로 broadcast됨\n",
        "print(\"x + y:\", x + y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdYcY1EI1Nd_",
        "outputId": "e0dbcdec-d4f0-4fa2-b595-3f66cedf713f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x + y: tensor([[3., 3., 3.],\n",
            "        [3., 3., 3.],\n",
            "        [3., 3., 3.],\n",
            "        [3., 3., 3.],\n",
            "        [3., 3., 3.]])\n"
          ]
        }
      ],
      "source": [
        "# PyTorch broadcasting: scalar to tensor\n",
        "# Similar to NumPy, PyTorch broadcasts the scalar value 2 to match x's shape\n",
        "x = torch.ones(5, 3)\n",
        "y = 2\n",
        "print(\"x + y:\", x + y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fl3QFw91Nd_",
        "outputId": "4a61b4fa-9f36-43cd-8d00-6ffa8f34b4fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x is tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]]), \n",
            "\n",
            "Shape of x is torch.Size([5, 3])\n",
            "\n",
            "y is tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]]), \n",
            "\n",
            "Shape of y is torch.Size([5, 1])\n"
          ]
        }
      ],
      "source": [
        "# PyTorch broadcasting: tensor to tensor with different shapes\n",
        "x = torch.ones(5, 3)\n",
        "y = torch.ones(5, 1)\n",
        "\n",
        "print(f\"x is {x}, \\n\\nShape of x is {x.shape}\\n\")\n",
        "print(f\"y is {y}, \\n\\nShape of y is {y.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PA6fw1lL1Nd_"
      },
      "source": [
        "#### Qn: Which dimension will y be broadcast along?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFF6wJaT1Nd_",
        "outputId": "1685cca6-cee3-49ed-b756-8c319bb7965e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x + y: tensor([[2., 2., 2.],\n",
            "        [2., 2., 2.],\n",
            "        [2., 2., 2.],\n",
            "        [2., 2., 2.],\n",
            "        [2., 2., 2.]])\n"
          ]
        }
      ],
      "source": [
        "# Here, y has shape (5,1) and is broadcast across the second dimension to match x's shape (5,3)\n",
        "# The value from each row in y is added to each element in the corresponding row of x\n",
        "print(\"x + y:\", x + y) # y는 5행 3열로 broadcast 된다\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "p5bMijTS1Nd_"
      },
      "outputs": [],
      "source": [
        "# # failed case of conflicting dimension\n",
        "# x = torch.ones(5, 3)\n",
        "# y = torch.ones(3, 5)\n",
        "# print(\"x + y:\", x + y)\n",
        "\n",
        "# 행과 열 둘중에 하나는 맞아야지 broadcast 된다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxMWCAkB54te"
      },
      "source": [
        "Indexing works as you would expect:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m39EwjI354te",
        "outputId": "9f6e84ca-e178-462b-dbad-455705e3ae3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.7388,  0.1480, -1.2032],\n",
            "        [ 0.2768, -0.1873,  0.5093],\n",
            "        [ 0.5512, -0.5932, -0.2209],\n",
            "        [ 0.4813, -0.7254, -0.8749],\n",
            "        [-0.1614,  0.3880,  2.4986]])\n",
            "tensor([[ 0.2768, -0.1873,  0.5093],\n",
            "        [ 0.5512, -0.5932, -0.2209],\n",
            "        [ 0.4813, -0.7254, -0.8749]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.randn(5, 3)\n",
        "print(x)\n",
        "print(x[1:4, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eV_vZgX30bft",
        "outputId": "b6d67a12-7c37-4f64-8141-3244054a3cf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.7388, 0.1480, 0.0000],\n",
            "        [0.2768, 0.0000, 0.5093],\n",
            "        [0.5512, 0.0000, 0.0000],\n",
            "        [0.4813, 0.0000, 0.0000],\n",
            "        [0.0000, 0.3880, 2.4986]])\n"
          ]
        }
      ],
      "source": [
        "# indexing by operators\n",
        "x[x < 0] = 0\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipHSy8b-gv0e"
      },
      "source": [
        "You can change the order of the dimensions of a tensor with `torch.permute()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrYUa-7ag1LG",
        "outputId": "ad1b8215-3efc-4ff1-d01b-2419c15f320b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x is tensor([[[-0.2909,  0.7799],\n",
            "         [ 1.8114,  0.9891],\n",
            "         [ 0.4329, -0.2814]],\n",
            "\n",
            "        [[-1.0368, -1.0650],\n",
            "         [-0.0837,  1.2157],\n",
            "         [ 0.4814, -0.9598]],\n",
            "\n",
            "        [[-0.0087, -0.6812],\n",
            "         [-1.2761,  1.1642],\n",
            "         [-0.9655,  0.9323]],\n",
            "\n",
            "        [[ 0.8809, -0.9325],\n",
            "         [-0.0666,  1.1696],\n",
            "         [-0.6376, -0.3542]],\n",
            "\n",
            "        [[-0.3159,  0.3105],\n",
            "         [ 0.9525,  0.3854],\n",
            "         [-0.5360,  0.7682]]]), \n",
            "\n",
            "Shape of x is torch.Size([5, 3, 2])\n"
          ]
        }
      ],
      "source": [
        "# Create a 3D tensor with random values from a normal distribution\n",
        "# Shape (5, 3, 2) means:\n",
        "#   - 5 elements in the first dimension (think of as \"layers\") == Index 0\n",
        "#   - 3 elements in the second dimension (think of as \"rows\") == Index 1\n",
        "#   - 2 elements in the third dimension (think of as \"columns\") == Index 2\n",
        "x = torch.randn(5, 3, 2) # 3행 2열의 5차원 torch 생성\n",
        "print(f\"x is {x}, \\n\\nShape of x is {x.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSXUftrN1NeF"
      },
      "source": [
        "#### Qn: What would permute(1,2,0) avhieve? Is the data still the same?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpAAdnsw1NeF",
        "outputId": "51a2373c-54b0-4483-f36a-36e7cfd96eb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x.permute(1,2,0) is:\n",
            "tensor([[[-0.2909, -1.0368, -0.0087,  0.8809, -0.3159],\n",
            "         [ 0.7799, -1.0650, -0.6812, -0.9325,  0.3105]],\n",
            "\n",
            "        [[ 1.8114, -0.0837, -1.2761, -0.0666,  0.9525],\n",
            "         [ 0.9891,  1.2157,  1.1642,  1.1696,  0.3854]],\n",
            "\n",
            "        [[ 0.4329,  0.4814, -0.9655, -0.6376, -0.5360],\n",
            "         [-0.2814, -0.9598,  0.9323, -0.3542,  0.7682]]])\n",
            "\n",
            "Shape of x.permute(1,2,0) is: torch.Size([3, 2, 5])\n"
          ]
        }
      ],
      "source": [
        "# Ans: no values are changed, only how they're organized in memory and how you access them!\n",
        "# permute rearranges the dimensions of a tensor : permute는 tensor의 실제 데이터 값을 바꾸는게 아니라 읽는 순서만 바꾸는 메서드\n",
        "# permute(1,2,0) changes the order of dimensions from (0,1,2) to (1,2,0)\n",
        "# 위의 결과를 보면 5개 차원의 3행 2열 행렬로 읽었는데,\n",
        "# 아래 결과를 보면 3개 차원의 2행 5열 행렬로 읽는다.\n",
        "# This is like transposing a matrix, but generalized to higher dimensions\n",
        "\n",
        "print(f\"x.permute(1,2,0) is:\\n{x.permute(1,2,0)}\\n\")\n",
        "print(f\"Shape of x.permute(1,2,0) is: {x.permute(1,2,0).shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBccd2N7Mp4_"
      },
      "source": [
        "Tensor data types and casting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v52347k654th",
        "outputId": "4906feae-4b94-4f20-81d6-60b26b084555"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]], dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "# torch.double is equivalent to torch.float64 (64-bit floating point)\n",
        "a = torch.ones(3, 3, dtype=torch.double)\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVlmY6tO1NeF",
        "outputId": "bda4ab5b-1089-4dee-9601-6108700aec65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b is tensor([[1, 1, 1],\n",
            "        [1, 1, 1],\n",
            "        [1, 1, 1]]), \n",
            "\n",
            "Type of b is torch.LongTensor\n",
            "c is tensor([[1, 1, 1],\n",
            "        [1, 1, 1],\n",
            "        [1, 1, 1]], dtype=torch.int32), \n",
            "\n",
            "Type of c is torch.IntTensor\n",
            "a is tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]], dtype=torch.float64), \n",
            "\n",
            "Type of a is torch.DoubleTensor\n"
          ]
        }
      ],
      "source": [
        "# Converting tensor to long (int64) data type\n",
        "# .long() is a convenience method equivalent to a.to(torch.int64)\n",
        "b = a.long()\n",
        "print(f\"b is {b}, \\n\\nType of b is {b.type()}\")\n",
        "\n",
        "# Converting tensor to int (int32) data type\n",
        "# .int() is a convenience method equivalent to a.to(torch.int32)\n",
        "c = a.int()\n",
        "print(f\"c is {c}, \\n\\nType of c is {c.type()}\")\n",
        "\n",
        "# check value of a again! 타입 캐스팅 시 깊은 복사가 일어나서 새로운 메모리 공간이 할당된다. 즉 a가 가리키는 메모리랑 b, c가 가리키는 메모리가 다름 (새로운 텐서 반환)\n",
        "print(f\"a is {a}, \\n\\nType of a is {a.type()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9jROw58rvVZ"
      },
      "source": [
        "Documentation on various dtypes: https://pytorch.org/docs/stable/tensors.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "strIC2QY54tk"
      },
      "source": [
        "### More useful PyTorch Tensor operations\n",
        "\n",
        "To see the complete API check here: https://pytorch.org/docs/stable/tensors.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tWE7sT0uvue"
      },
      "source": [
        "`.view()` can be used to resize/reshape tensors:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXwIvBQS54tk",
        "outputId": "b0aab8ac-7c49-4ffd-ca9f-c47cce0a0251"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
          ]
        }
      ],
      "source": [
        "x = torch.randn(4, 4)\n",
        "y = x.view(16) # 16개의 열로 변환\n",
        "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions (-1 넣으면 알아서 계산해서 정해준다는 이야기)\n",
        "print(x.size(), y.size(), z.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYTgOyt5u1ph"
      },
      "source": [
        "If you have a one element tensor, use `.item()` to get the value as a Python number:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-rltnVuu69S",
        "outputId": "e34827b3-564c-402f-ec94-35352cab8b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(20.)\n",
            "20.0\n"
          ]
        }
      ],
      "source": [
        "x = torch.ones(4,5)\n",
        "x = x.sum()\n",
        "print(x)\n",
        "print(x.item()) # item()을 사용하면 Python 자료구조 형태로 바꿔준다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV_tHHW85m-U"
      },
      "source": [
        "Concatenating two matrices together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7L-sqwK5mOA",
        "outputId": "7057a214-4fe3-4a09-8903-ad23496f49bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.ones(5, 3)\n",
        "y = torch.zeros(5, 2)\n",
        "print(torch.cat([x, y], dim=1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnSnOy3C1NeG"
      },
      "source": [
        "#### Qn: What happens if you try to concatenate along dim=0?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePrHECAT1NeG",
        "outputId": "ca4859ee-aed7-439a-fd7b-04054cd1c1ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RuntimeError: 행을 제외한 나머지 차원이 다르면 안돼요\n",
            "tensor([[1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "# # Ans: RuntimeError: Sizes of tensors must match except in dimension 0:\n",
        "try:\n",
        "    print(torch.cat([x, y], dim=0))\n",
        "except Exception :\n",
        "    print(\"RuntimeError: 행을 제외한 나머지 차원이 다르면 안돼요\")\n",
        "# x는 5행 3열, y는 5행 2열\n",
        "# dim=0이라는건, 첫번째 축 방향(행)으로 이어붙이기 떄문에, 차원 0을 제외한 나머지 차원이 일치해야 한다.\n",
        "# 이 경우, x와 y의 두번째 출 방향(열)이 다르므로 오류가 발생한다.\n",
        "print(torch.cat([x, y], dim=1)) # 그럼 이 경우는 가능하다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y023N4uw54tm"
      },
      "source": [
        "### Converting between NumPy arrays and PyTorch Tensors\n",
        "\n",
        "Important: PyTorch Tensors and NumPy arrays will share the same underlying memory locations. If you change values for one, the values for the other will be changed too."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3VKumG9ump-"
      },
      "source": [
        "Convert PyTorch Tensor to NumPy array:\n",
        "\n",
        "such conversion requires source tensor to be on CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xa88YHL54tn",
        "outputId": "108cabd7-a2ea-4b5b-f43d-6ac94d98d464"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a is tensor([1., 1., 1., 1., 1.])\n",
            "b is [1. 1. 1. 1. 1.]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2., 2., 2., 2., 2.])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "a = torch.ones(5)\n",
        "print(f\"a is {a}\")\n",
        "\n",
        "# Convert PyTorch tensor to NumPy array\n",
        "# Note: This conversion only works for source tensors on CPU, not for GPU tensors\n",
        "# 텐서를 numpy 배열로 바꿀때는 .numpy()사용\n",
        "b = a.numpy() # 얕은 복사 수행 (b가 a랑 동일한 메모리를 가리킴)\n",
        "print(f\"b is {b}\")\n",
        "\n",
        "# Perform an in-place addition of 1 to the tensor\n",
        "a.add_(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oe9wZrJY1NeG",
        "outputId": "714b564a-86de-419b-a47a-a20f2fbfcdbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a is tensor([2., 2., 2., 2., 2.])\n",
            "b is [2. 2. 2. 2. 2.]\n"
          ]
        }
      ],
      "source": [
        "# Both the tensor and array now contain 2's instead of 1's\n",
        "# This demonstrates that the tensor and array share the same underlying memory\n",
        "print(f\"a is {a}\")  # PyTorch tensor is modified\n",
        "print(f\"b is {b}\")  # NumPy array is also modified because of shared memory!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMIhgJgivCsm"
      },
      "source": [
        "Convert NumPy array to PyTorch Tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55AFarlO54tp",
        "outputId": "62b664f6-145b-4e5d-e5c9-f6edfe80ab4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n",
            "[1. 1. 1. 1. 1.]\n",
            "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "#반대로 넘파이를 텐서로 바꾸고 싶다면\n",
        "a = np.ones(5)\n",
        "b = torch.from_numpy(a) # .numpy(), .from_numpy() => 얕은 복사 수행\n",
        "print(type(b))\n",
        "print(a)\n",
        "print(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw5RXmXQ1NeG"
      },
      "source": [
        "#### Qn: What happens if I run the cell below twice?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhXGKdwU1NeG",
        "outputId": "3f8eecc2-a28e-4165-c79b-e6b88a1e7434"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a is [2. 2. 2. 2. 2.]\n",
            "b is tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "# Perform in-place addition on the NumPy array\n",
        "# The 'out=a' parameter means the result is stored back in 'a' (in-place operation)\n",
        "np.add(a, 1, out=a) # add_는 pytorch 메서드 -> numpy에서는 add()의 parameter로 out=대상 지정\n",
        "\n",
        "# Both the NumPy array and PyTorch tensor now contain 2's instead of 1's\n",
        "# This demonstrates the shared memory between PyTorch tensors and NumPy arrays\n",
        "print(f\"a is {a}\")\n",
        "print(f\"b is {b}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gg90vky-54tr"
      },
      "source": [
        "## CUDA Tensors (On GPU)\n",
        "\n",
        "PyTorch tensors have the added benefit that they can easily be placed on a GPU to speed up computations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_Lfnh2wPeQz"
      },
      "source": [
        "Query information about the GPU (if CUDA is available):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0i9K6ucxPLZr",
        "outputId": "6cca42f8-d5f9-48bd-8a4f-431b6c5b27d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue May 13 12:07:03 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    !nvidia-smi # GPU 확인 ubuntu 명령어"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiEkS3ZjP5aH"
      },
      "source": [
        "You can use `torch.device` objects to move tensors to and from the GPU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2ue8FEc54tr",
        "outputId": "833cd3da-6360-4295-bc3b-64e99325584f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 2., 2.],\n",
            "        [2., 2., 2.],\n",
            "        [2., 2., 2.],\n",
            "        [2., 2., 2.],\n",
            "        [2., 2., 2.]], device='cuda:0')\n",
            "tensor([[2., 2., 2.],\n",
            "        [2., 2., 2.],\n",
            "        [2., 2., 2.],\n",
            "        [2., 2., 2.],\n",
            "        [2., 2., 2.]], dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")          # a CUDA device object\n",
        "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU (GPU 버전의 tensor 생성 방법)\n",
        "    x = x.to(device)                       # or just use strings `.to(\"cuda\")`\n",
        "    z = x + y\n",
        "    print(z)\n",
        "    print(z.to(\"cpu\", torch.double))       # `.to` can also change dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzlpOvUg64mB"
      },
      "source": [
        "Default device can be specified by [`torch.cuda.set_device(device)`](https://pytorch.org/docs/stable/generated/torch.cuda.set_device.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXF6Ykbr54tu"
      },
      "source": [
        "\n",
        "## Autograd: Automatic Differentiation\n",
        "\n",
        "From: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n",
        "\n",
        "Now that you have learned how to use PyTorch Tensors you will learn how we can use PyTorch for automatic differentiation.\n",
        "\n",
        "The `autograd` package in PyTorch provides automatic differentiation for all operations on Tensors. It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different.\n",
        "\n",
        "To allow PyTorch to keep track of operations for automatic differentiation, we need to set `requires_grad` as `True` for a Tensor. Autograd will then start to track all operations on the Tensor. When you finish your computation you can call `.backward()` and have all the gradients computed automatically. The gradient for this tensor will be accumulated into the `.grad` attribute.\n",
        "\n",
        "To stop a tensor from tracking history, you can call `.detach()` to detach\n",
        "it from the computation history, and to prevent future computation from being\n",
        "tracked.\n",
        "\n",
        "To prevent tracking history (and using memory), you can also wrap the code block in `with torch.no_grad():`. This can be particularly helpful when evaluating a model because the model may have trainable parameters with `requires_grad=True`, but for which we don't need the gradients.\n",
        "\n",
        "There’s one more class which is very important for autograd implementation - a `Function`.\n",
        "\n",
        "`Tensor` and `Function` are interconnected and build up an acyclic\n",
        "graph that encodes a complete history of computation. Each tensor has\n",
        "a `.grad_fn` attribute that references a `Function` that has created\n",
        "the `Tensor` (except for Tensors created by the user - their `grad_fn` is `None`).\n",
        "\n",
        "If you want to compute the derivatives, you can call `.backward()` on a `Tensor`. If `Tensor` is a scalar (i.e. it holds a one element data), you don’t need to specify any arguments to `backward()`, however if it has more elements, you need to specify a `gradient` argument that is a tensor of matching shape."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "autograd : Pytorch 자동 미분 엔진\n",
        "Tensor 연산을 추적해서, .backward()를 호출하면 자동으로 gradient를 계산해준다.\n",
        "\n",
        "특정 tensor에 대한 연산 기록을 추적하고 싶다면 requires_grad=True로 설정\n",
        "\n",
        "추적할 필요가 없다면 with torch.no_grad() 연산 블록으로 감싸주면 된다.  \n",
        "추적 중단 시 .detach() 함수를 사용해서 계산 history에서 분리할수도 있다.\n",
        "\n",
        "- Function : autograd 모듈은 내부적으로 각 연산을 Function 객체로 구현함\n",
        "- Function에는 순전파, 역전파 로직이 모두 포함되어 있음\n",
        "- y = x * 2라는 연산 수행 시 곱셈 연산 수행하는 MulBackward라는 Function 객체를 내부적으로 생성해서 기록함\n",
        "- Tensor는 Function과 연결되어서 계산 그래프(Directed acyclic graph - 순환 없는 그래프)를 구성하게 됨\n",
        "- 연산 결과로 생성된 Tensor에는 .grad_fn 속성을 가지게 되고, 어떤 Function객체가 이 Tensor를 생성했는지 알 수 있음"
      ],
      "metadata": {
        "id": "gEZMQzX5Jt7k"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_7reitR54tv"
      },
      "source": [
        "Create a tensor and set `requires_grad=True` to track computation with it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "BiUIjct_54tw",
        "outputId": "ac9b1b46-5ddb-48cb-96ce-4cd57f6264c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "x = torch.ones(2, 2, requires_grad=True)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceISCrjA54ty"
      },
      "source": [
        "Perform a tensor operation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "jol5D-i754tz",
        "outputId": "1166df21-f029-420b-da6d-844e2c2ebbf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[3., 3.],\n",
            "        [3., 3.]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "y = x + 2\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELfAiQbe54t1"
      },
      "source": [
        "`y` was created as a result of an operation, so it has a `grad_fn`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "N1-bfsGg54t1",
        "outputId": "36d8c6ca-4a89-4c51-d981-f39bb7623f38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<AddBackward0 object at 0x7b8d688ea680>\n"
          ]
        }
      ],
      "source": [
        "print(y.grad_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKGpA_tx54t3"
      },
      "source": [
        "Do more operations on `y`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ZEr955CF54t3",
        "outputId": "6b871a33-fe6f-4b58-cbbf-7f95e42973d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[27., 27.],\n",
            "        [27., 27.]], grad_fn=<MulBackward0>)\n",
            "tensor(27., grad_fn=<MeanBackward0>)\n"
          ]
        }
      ],
      "source": [
        "z = y * y * 3\n",
        "print(z)\n",
        "out = z.mean()\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_W_IdTf054t5"
      },
      "source": [
        "`.requires_grad_()` changes an existing Tensor's `requires_grad` flag in-place. The input flag defaults to `False` if not given:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8jGQeJX1NeH"
      },
      "source": [
        "#### Qn: What if you initialize a tensor and want to track"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Mqhs7laq54t5",
        "outputId": "e280e7c6-1bca-4957-cb91-36f45d0fff40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor 'a' requires_grad: False\n"
          ]
        }
      ],
      "source": [
        "a = torch.randn(2, 2)\n",
        "\n",
        "# element-wise operations\n",
        "a = ((a * 3) / (a - 1))\n",
        "\n",
        "# Check if tensor 'a' is set up to track gradients (for automatic differentiation)\n",
        "# By default, tensors don't track gradients to save memory and computation\n",
        "print(f\"Tensor 'a' requires_grad: {a.requires_grad}\") # Default로 requires_grad=False로 설정됨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Aelnwdwu1NeH",
        "outputId": "0cb68991-4540-4f90-8bc5-06150a21585b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor 'a' requires_grad: True\n",
            "b.grad_fn: <SumBackward0 object at 0x7b8d688e9f60>\n"
          ]
        }
      ],
      "source": [
        "# Enable gradient tracking for tensor 'a'\n",
        "# This is necessary when you want to compute derivatives with respect to this tensor\n",
        "# The underscore in requires_grad_() indicates this method modifies the tensor in-place\n",
        "a.requires_grad_(True) # 이 시점부터 계산 history가 기록된다\n",
        "print(f\"Tensor 'a' requires_grad: {a.requires_grad}\")\n",
        "\n",
        "# Create a new tensor 'b' by performing element-wise operations on 'a'\n",
        "# When operations are performed on tensors with requires_grad=True,\n",
        "# PyTorch builds a computational graph to track operations for backpropagation\n",
        "b = (a * a).sum() # 가장 마지막에 SumBackward0이 수행됨\n",
        "\n",
        "# grad_fn shows the last operation used to create this tensor\n",
        "print(f\"b.grad_fn: {b.grad_fn}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "tavzi20h1NeH",
        "outputId": "ab9c070f-ddb6-43e5-9156-64bb11a04569",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-33.3527,   1.8652],\n",
            "        [  2.4232,   3.1865]])\n"
          ]
        }
      ],
      "source": [
        "b.backward()  # Compute gradients (역전파 수행)\n",
        "\n",
        "# Now 'a' would have its gradient populated\n",
        "print(a.grad)  # Shows dL/da - the gradient of b with respect to a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmHvopdf54t7"
      },
      "source": [
        "### Gradients\n",
        "\n",
        "Let's backprop now. Because `out` contains a single scalar, `out.backward()` is equivalent to `out.backward(torch.tensor(1))`:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "AAHwiVuB54t8"
      },
      "outputs": [],
      "source": [
        "out.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-PJNi8j54t-"
      },
      "source": [
        "Print gradients $\\frac{d(\\texttt{out})}{d\\texttt{x}}$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ZMFshmPS54t-",
        "outputId": "45df1cfc-c315-4f50-a63e-178970e5311e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[4.5000, 4.5000],\n",
            "        [4.5000, 4.5000]])\n"
          ]
        }
      ],
      "source": [
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISxLOt0d54uB"
      },
      "source": [
        "You should get a matrix of `4.5`. Let's call the `out` *Tensor* \"$o$\". We find that $o = \\frac{1}{4}\\sum_i z_i$, $z_i = 3(x_i+2)^2$ and $z_i\\bigr\\rvert_{x_i=1} = 27$. Therefore, $\\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2)$, hence $\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOnU_3Q_54uB"
      },
      "source": [
        "You can do many crazy things with autograd!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "u7Vy1rJr54uB",
        "outputId": "4aae23b5-b6aa-49c4-971d-9239b2f3253c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1019.6857, -1280.7091,  -136.9754], grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# requires_grad=True enables automatic differentiation for tensor x\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "\n",
        "# This operation is tracked in the computational graph since x requires gradients\n",
        "y = x * 2\n",
        "\n",
        "while y.data.norm() < 1000:   ## y.data gives a copy of y's values without gradient tracking\n",
        "    y = y * 2\n",
        "\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVP3Rbza1NeI"
      },
      "source": [
        "#### Quick recap: When calling .backward() on a non-scalar tensor, we must provide an external gradient that represents \"how some scalar value changes with respect to each element of y\" (∂L/∂y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "-wN5uMku54uE",
        "outputId": "d4e6a855-04a0-41bf-c43a-9a855bbdc086",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
          ]
        }
      ],
      "source": [
        "# Create a gradient tensor that represents ∂L/∂y for some hypothetical scalar function L\n",
        "# The values [0.1, 1.0, 0.0001] represent ∂L/∂y₁, ∂L/∂y₂, and ∂L/∂y₃ respectively\n",
        "# if y contributes to some scalar value L, these are the ∂L/∂y values\n",
        "gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
        "\n",
        "# Perform backpropagation through the computational graph\n",
        "# PyTorch uses the chain rule: ∂L/∂x = (∂L/∂y)(∂y/∂x)\n",
        "# where ∂L/∂y is our provided gradient tensor\n",
        "y.backward(gradients)\n",
        "\n",
        "# Print the gradient of x\n",
        "# This shows ∂L/∂x - the gradient of our hypothetical scalar function with respect to x\n",
        "# The values combine: (how our scalar output changes with y) x (how y changes with x)\n",
        "# This is the chain rule in action: ∂L/∂x = (∂L/∂y)(∂y/∂x)\n",
        "print(x.grad)\n",
        "\n",
        "# THINK: multi-dimensional output but need gradients for optimization of a single\n",
        "# scalar quantity such as a loss function!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3WfxSbS54uF"
      },
      "source": [
        "You can also stop autograd from tracking history on Tensors with `.requires_grad=True` by wrapping the code block in `with torch.no_grad()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "FxbhHk8654uG",
        "outputId": "4a5b62d4-4f2c-409a-dabf-69fcff6f42b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "print(x.requires_grad)\n",
        "print((x ** 2).requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "\tprint((x ** 2).requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9Ig0EXKOGac"
      },
      "source": [
        "The `.detach()` method is used when we want to perform operations on tensors without affecting the gradient computation of the original tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "CxE_GcDmOROi",
        "outputId": "101dc417-7f09-4452-de20-7a2607a06052",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "x = torch.randn(3, requires_grad=True)\n",
        "print(x.requires_grad)\n",
        "\n",
        "y = x.detach() # 성능만 측정 시 불필요한 gradient 계산을 방지하기 위해 detach() 함수를 사용한다. (validation, testing of a model)\n",
        "print(y.requires_grad)\n",
        "\n",
        "z = y * 2\n",
        "print(z.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzwqx4hZ1NeI"
      },
      "source": [
        "#### Qn: Which scenarios would benefit from .detach()?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "Y0amMdGZ1NeI"
      },
      "outputs": [],
      "source": [
        "# Ans: Validation and testing of a model\n",
        "# .detach() can help save memory by not tracking gradients on the validation/test set calculations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtHPCO3R54uI"
      },
      "source": [
        "**Read Later:**\n",
        "\n",
        "Documentation of `torch.autograd` and `Function` is at\n",
        "http://pytorch.org/docs/autograd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDKBSQ9n54uJ"
      },
      "source": [
        "## Neural Networks\n",
        "\n",
        "The `torch.nn` package in PyTorch provides higher level building blocks for neural networks like fully connected or convolutional layers. The `nn` package makes use of the `autograd` functionality to define these model building blocks and differentiate them. This allows us to quickly and easily implement neural networks by putting together layers and using PyTorch to help us update learnable parameters with the gradient.\n",
        "\n",
        "An `nn.Module` contains layers, and a method `forward(input)` that\n",
        "returns the `output`.\n",
        "\n",
        "A typical training procedure for a neural network is as follows:\n",
        "\n",
        "- Define the neural network that has some learnable parameters (or\n",
        "  weights)\n",
        "- Iterate over a dataset of inputs\n",
        "- Process input through the network\n",
        "- Compute the loss (how far is the output from being correct)\n",
        "- Propagate gradients back into the network’s parameters\n",
        "- Update the weights of the network, typically using a simple update rule:\n",
        "  `weight = weight - learning_rate * gradient`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szNq6Hk854uJ"
      },
      "source": [
        "### Define the network\n",
        "\n",
        "In the cell below we define a simple convolutional neural network. Notice that we use the `nn.Conv2d` and `nn.Linear` Modules as building blocks for the network.\n",
        "\n",
        "There are plenty of other types of layers and tools available in the [torch.nn](https://pytorch.org/docs/stable/nn.html) package such as pooling layers, dropout, and batchnorm.\n",
        "\n",
        "Conveniently, PyTorch is completely open source so you can check out exactly how each of these Modules are implemented:\n",
        "\n",
        "* https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/linear.py\n",
        "* https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/conv.py\n",
        "\n",
        "**Important:** Whenever you extend the `nn.Module` class (e.g. with the `Net` class below) you will need to call the superclass constructor or an error will be thrown. In this example below this line is: `super().__init__()`"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nn.Module : Pytorch에서 사용되는 모든 신경망 모델의 기반 클래스  \n",
        "신경망을 구성하는 레이어, 가중치, 파라미터, forward 로직을 캡슐화한다.  \n",
        "사용자 정의 모델을 만들기 위해서는 항상 nn.Module을 상속해야 하고  \n",
        "super().__init__()을 반드시 호출해야 한다.\n"
      ],
      "metadata": {
        "id": "I_ivts8KQtfW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "A6MyjNHm54uK",
        "outputId": "55b077a8-051b-478f-cf41-33e6e03cc861",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self): # 초기화 수행 및 어떤 연산 수행할건지 정의하는 곳\n",
        "        super().__init__() # 반드시 있어야함\n",
        "        # Convd : 2D Convolution Layer, 이미지같은 2차원 데이터를 처리하는 클래스\n",
        "\n",
        "        # 1 input image channel, 6 output channels, 5x5 convolution kernel\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5) # 예를 들어 32x32 image -> 5x5 커널 적용 시 -> 32 - 5 + 1 = 28x28 크기의 6개의 채널이 구성됨\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5) # 6개의 입력채널, 16개의 출력채널, 5x5 커널\n",
        "\n",
        "        # Fully connected layer -> CNN 동작을 통해 특징을 추출하고, 이 특징들을 바탕으로 의사 결정을 수행하는 역할 (판단을 내리는 역할)\n",
        "        # Linear -> Affine operation: y = Wx + b 수행\n",
        "        # nn.Linear(입력특징, 출력특징)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120) # 총 특징 개수가 16 * 5 * 5 이니까 입력으로 16 * 5 * 5\n",
        "        self.fc2 = nn.Linear(120, 84) # fc1에서 출력 결과가 120개 -> 84개로 변환\n",
        "        self.fc3 = nn.Linear(84, 10) # 최종적으로 반환되는 특징은 10개\n",
        "\n",
        "    def forward(self, x): # 입력을 받아 출력으로 변환하는 연산을 정의하는 함수\n",
        "        # x : 입력\n",
        "        # Max pooling over a (2, 2) window\n",
        "        # ReLU : 활성화함수 Relu(x) = max(0, x) -> 비선형성을 부여하는 활성화 함수\n",
        "        # 음수를 0으로 만들어버리고 양수만 전달하는 활성화함수\n",
        "\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), 2) # conv1로 이미지의 특징을 추출 -> maxpool2d로 downsampling (특징 압축)\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2) # conv2로 또 이미지의 특징을 추출 -> maxpool2d로 downsampling\n",
        "\n",
        "        x = x.flatten(start_dim=1) # CNN 출력 - FC층의 입력으로 바꾸기 위해 reshape\n",
        "        # Conv2D는 [B, C, H, W] 형태의 4차원 텐서를 출력함 (Batch : 한번에 처리할 샘플 개수, C : 채널 개수 (RGB), H/W : 행/열)\n",
        "        # 이를 FC층에 넣기 위해 [B, C*H*W] 형태로 변환함 (batch dimension은 그대로 유지)\n",
        "\n",
        "\n",
        "        x = F.relu(self.fc1(x)) # Affine 연산 1\n",
        "        x = F.relu(self.fc2(x)) # Affine 연산 2\n",
        "        x = self.fc3(x) # Affine 연산 3\n",
        "        return x # 최종\n",
        "\n",
        "\n",
        "net = Net()\n",
        "print(net) # 출력 시 어떤 연산을 수행하는지 알 수 있다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8_soKaq54uM"
      },
      "source": [
        "You just have to define the ``forward`` function, and the ``backward``\n",
        "function (where gradients are computed) is automatically defined for you\n",
        "using ``autograd``.\n",
        "You can use any of the Tensor operations in the ``forward`` function.\n",
        "\n",
        "The learnable parameters of a model are returned by ``net.parameters()``\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "kNW8772A54uN",
        "outputId": "10eab3f9-a392-4709-de2a-d5912bc764f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "torch.Size([6, 1, 5, 5])\n",
            "######\n",
            "torch.Size([6, 1, 5, 5])\n",
            "torch.Size([6])\n",
            "torch.Size([16, 6, 5, 5])\n",
            "torch.Size([16])\n",
            "torch.Size([120, 400])\n",
            "torch.Size([120])\n",
            "torch.Size([84, 120])\n",
            "torch.Size([84])\n",
            "torch.Size([10, 84])\n",
            "torch.Size([10])\n"
          ]
        }
      ],
      "source": [
        "params = list(net.parameters()) # parameters() : 정의한 모델에 있는 모든 학습 가능한 파라미터(weight, bias)를 리스트로 반환하는 함수\n",
        "print(len(params)) # 파라미터의 개수가 10개 -> conv1, 2 / fc1,2,3 각각 weight, bias 존재 -> 총 10개\n",
        "print(params[0].size())  # conv1's .weight\n",
        "print(\"######\")\n",
        "for idx, param in enumerate(params):\n",
        "    # CNN : [출력size, 입력size, kernel, kernel]\n",
        "    # FCLayer : [출력 feature 개수, 입력 feature 개수]\n",
        "    print(param.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "wFKPw77D1NeJ",
        "outputId": "791b558a-1614-4c8d-954b-342cdc9d03b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv 1 layer weights are Parameter containing:\n",
            "tensor([[[[ 0.1221, -0.1280,  0.1798, -0.0202,  0.1807],\n",
            "          [ 0.1091, -0.0316,  0.0695,  0.1229, -0.0229],\n",
            "          [-0.0526, -0.1294, -0.0584,  0.1641,  0.1675],\n",
            "          [ 0.1625,  0.0759,  0.0770,  0.0206, -0.1326],\n",
            "          [-0.1009,  0.1775, -0.1002,  0.0414,  0.0307]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0248, -0.1425,  0.0401,  0.0297, -0.0737],\n",
            "          [ 0.1874, -0.1720,  0.0805, -0.0829, -0.1219],\n",
            "          [-0.0072, -0.0985, -0.1724,  0.0085,  0.0666],\n",
            "          [ 0.1592,  0.0183,  0.0631,  0.0827, -0.0924],\n",
            "          [-0.1347,  0.1975, -0.0390,  0.0225, -0.0648]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0135, -0.1353,  0.0907,  0.0397,  0.0389],\n",
            "          [ 0.1011, -0.1470,  0.0759, -0.1650, -0.1758],\n",
            "          [ 0.1317, -0.0827,  0.0468, -0.1928, -0.1933],\n",
            "          [ 0.0490, -0.0830, -0.0788, -0.1772,  0.1372],\n",
            "          [ 0.0130,  0.0612,  0.0343, -0.0215, -0.1948]]],\n",
            "\n",
            "\n",
            "        [[[-0.1174, -0.0852,  0.1928, -0.0476,  0.0841],\n",
            "          [ 0.0428,  0.0857,  0.0065, -0.1891, -0.0396],\n",
            "          [-0.0404,  0.1285, -0.0551, -0.0166,  0.1547],\n",
            "          [-0.0948,  0.1261, -0.1707,  0.1263,  0.0197],\n",
            "          [-0.1145,  0.0181,  0.1416, -0.1916, -0.0624]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1839, -0.0821,  0.1396, -0.1061,  0.0893],\n",
            "          [ 0.0164, -0.0084,  0.1113,  0.1480,  0.1900],\n",
            "          [-0.0333,  0.1432, -0.0963, -0.0721,  0.1568],\n",
            "          [ 0.0390,  0.0392,  0.1131,  0.0345,  0.0766],\n",
            "          [-0.0110,  0.1411, -0.0737,  0.1691, -0.0551]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1297, -0.0571, -0.1888, -0.1229,  0.1863],\n",
            "          [ 0.1051,  0.1103, -0.0659, -0.0468,  0.0941],\n",
            "          [ 0.0544, -0.1409,  0.0465,  0.0483, -0.0876],\n",
            "          [ 0.1435,  0.1023, -0.1203,  0.0937,  0.1040],\n",
            "          [-0.0060,  0.1016, -0.1676,  0.0982,  0.1239]]]], requires_grad=True)\n",
            "conv 1 layer biases are Parameter containing:\n",
            "tensor([-0.1092,  0.1791,  0.0111, -0.0367,  0.1435, -0.0890],\n",
            "       requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "# Let us look at all of the conv1 weights\n",
        "# Note: Conv1 layer has 6 filters, each of size 5x5\n",
        "print(f\"conv 1 layer weights are {params[0]}\") # conv1.weight\n",
        "# Let us look at all of the conv1 biases\n",
        "# Note: Conv1 layer has 6 filters, each with a bias\n",
        "print(f\"conv 1 layer biases are {params[1]}\") # conv1.bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuMk3eX654uP"
      },
      "source": [
        "Continuing, let's try a random 32x32 input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "3VGmILUs54uP",
        "outputId": "c8967816-2237-4cce-f902-5e41a7fc84a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0899,  0.0868, -0.0371, -0.0106, -0.1570,  0.0983, -0.0880, -0.0053,\n",
            "          0.0032, -0.0212]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# [1 sample, 1 channel, 32 height, 32 width]\n",
        "input = torch.randn(1, 1, 32, 32)\n",
        "\n",
        "# This will correctly apply dropout during training:\n",
        "out = net(input) # nn.Module의 __call__() 메서드 호출 (dropout, batchnorm, hooks, ...) 모든 기능이 동작함\n",
        "print(out)\n",
        "# 학습이나 추론 시 net(input)으로 호출해야 한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "RdceMdjB1NeJ",
        "outputId": "93610420-fb32-46c7-fedf-5a2d58f605da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0899,  0.0868, -0.0371, -0.0106, -0.1570,  0.0983, -0.0880, -0.0053,\n",
              "          0.0032, -0.0212]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "# This might NOT correctly apply dropout or batch normalization because it bypasses __call__\n",
        "net.forward(input) # 단순히 forward만 호출함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C9Gy0Sy54uR"
      },
      "source": [
        "- Zero the gradient buffers of all parameters\n",
        "- To calculate the gradient of all the parameters that used to compute `out` w.r.t. some random value\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "jUb7RxZm54uR"
      },
      "outputs": [],
      "source": [
        "net.zero_grad()  # important, since gradient is accumulated / 모든 파라미터의 gradient를 0으로 초기화하는 함수\n",
        "# gradient를 매 epoch/batch마다 초기화해주지 않으면 이전 epoch에서 계산한 gradient가 현재 gradient에 영향을 미칠 수 있음\n",
        "# 따라서 매 반복마다 net.zero_grad()를 호출해서 초기화를 수행해야 함\n",
        "out.backward(torch.randn(1, 10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "nOkP3pBmSGjX",
        "outputId": "b51ce33c-d07b-400c-fcaa-73432037a079",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "# to check gradient buffer:\n",
        "net.conv1.bias.grad.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doU9r69emwm9"
      },
      "source": [
        "The [`nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) module can sometimes be helpful to define blocks succintly or avoid creating a new `nn.Module` class for a small network. The `.forward()` function will be automatically defined by running modules in the order they are passed in to `nn.Sequential`.\n",
        "\n",
        "For example, you can define a block of convolutional layers below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "N8y-QNNLnjmd"
      },
      "outputs": [],
      "source": [
        "conv_layers = nn.Sequential(\n",
        "                nn.Conv2d(1, 6, 5),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(6, 16, 5),\n",
        "                nn.ReLU()\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "wKv5EiAg1NeJ"
      },
      "outputs": [],
      "source": [
        "# Example\n",
        "# Complete LeNet-like CNN model using our conv_layers block\n",
        "class MnistCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MnistCNN, self).__init__()\n",
        "        # Reuse our predefined convolutional layers\n",
        "        self.features = conv_layers\n",
        "\n",
        "        # Add pooling layers and fully connected layers\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(16 * 5 * 5, 120),  # 16 channels, 5x5 feature maps\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(120, 84),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(84, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First convolutional block (changes size from 28x28 to 24x24, then to 20x20)\n",
        "        x = self.features(x)\n",
        "\n",
        "        # Pooling (reduces size from 20x20 to 10x10)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Flatten for the fully connected layers\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        # Classifier\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaONPht_54uT"
      },
      "source": [
        "**Note:**\n",
        "\n",
        "`torch.nn` only supports mini-batches. The entire `torch.nn` package only supports inputs that are a mini-batch of samples, and not a single sample.\n",
        "\n",
        "For example, `nn.Conv2d` will take in a 4D Tensor of `nSamples x nChannels x Height x Width`.\n",
        "\n",
        "If you have a single sample, just use `input.unsqueeze(0)` to add a fake batch dimension."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLLYpQ7O54uT"
      },
      "source": [
        "Before proceeding further, let's recap all the classes you’ve seen so far.\n",
        "\n",
        "**Recap:**\n",
        "- `torch.Tensor` - A *multi-dimensional array* with support for autograd operations like `backward()`. Also *holds the gradient* w.r.t. the tensor.\n",
        "- `nn.Module` - Neural network module. *Convenient way of encapsulating parameters*, with helpers for moving them to GPU, exporting, loading, etc.\n",
        "- `nn.Parameter` - A kind of Tensor, that is *automatically registered as a parameter when assigned as an attribute to a* `Module`.\n",
        "- `autograd.Function` - Implements *forward and backward definitions of an autograd operation*. Every `Tensor` operation, creates at least a single `Function` node, that connects to functions that created a `Tensor` and *encodes its history*.\n",
        "\n",
        "**At this point, we covered:**\n",
        "- Defining a neural network\n",
        "- Processing inputs and calling backward\n",
        "\n",
        "**Still Left:**\n",
        "- Computing the loss\n",
        "- Updating the weights of the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omNVq_XZ2KV5"
      },
      "source": [
        "### Loss Function\n",
        "\n",
        "A loss function takes the (output, target) pair of inputs, and computes a\n",
        "value that estimates how far away the output is from the target.\n",
        "\n",
        "There are several different [loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions) under the `nn` package. A simple loss is `nn.MSELoss`, which computes the mean-squared error between the input and the target.\n",
        "\n",
        "For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "uMUJ2zAO54uU",
        "outputId": "83c78513-44e4-4aff-8dc2-877f7d74ac6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.6461, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "output = net(input)\n",
        "target = torch.randn(10)  # a dummy target, for example\n",
        "target = target.view(1, -1)  # make it the same shape as output\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "loss = criterion(output, target)\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85d4XN3W54uW"
      },
      "source": [
        "Now, if you follow `loss` in the backward direction, using its `.grad_fn` attribute, you will see a graph of computations that looks like this:\n",
        "\n",
        "    input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
        "          -> view -> linear -> relu -> linear -> relu -> linear\n",
        "          -> MSELoss\n",
        "          -> loss\n",
        "\n",
        "So, when we call `loss.backward()`, the whole graph is differentiated w.r.t. the loss, and all Tensors in the graph that have `requires_grad=True` will have their `.grad` Tensor accumulated with the gradient.\n",
        "\n",
        "For illustration, let us follow a few steps backward:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "am1pApy254uW",
        "outputId": "9f33892f-2e0e-4c60-f8a0-52c322400d92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<MseLossBackward0 object at 0x7b8d2378cdf0>\n",
            "<AddmmBackward0 object at 0x7b8d2378f130>\n",
            "<ReluBackward0 object at 0x7b8d2378cc40>\n"
          ]
        }
      ],
      "source": [
        "print(loss.grad_fn)  # MSELoss\n",
        "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
        "print(loss.grad_fn.next_functions[0][0].next_functions[1][0])  # ReLU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRqVnKtx54uY"
      },
      "source": [
        "### Backprop\n",
        "\n",
        "To backpropagate the error all we have to do is call `loss.backward()`. You need to clear the existing gradients though, otherwise the gradients will be accumulated to existing gradients.\n",
        "\n",
        "Now we'll call `loss.backward()`, and have a look at conv1's bias\n",
        "gradients before and after the backward step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "7u45jgvE54uZ",
        "outputId": "6a1c0f26-6b2a-4a23-930a-f2c5f9c4d57e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.bias.grad before backward\n",
            "None\n",
            "conv1.bias.grad after backward\n",
            "tensor([ 0.0113,  0.0040, -0.0053, -0.0078,  0.0167, -0.0042])\n"
          ]
        }
      ],
      "source": [
        "net.zero_grad()  # zeroes the gradient buffers of all parameters\n",
        "\n",
        "print('conv1.bias.grad before backward')\n",
        "print(net.conv1.bias.grad)\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print('conv1.bias.grad after backward')\n",
        "print(net.conv1.bias.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzHCWS8554ub"
      },
      "source": [
        "Now, we have seen how to use loss functions.\n",
        "\n",
        "**The only thing left to learn is:**\n",
        "\n",
        "- Updating the weights of the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxIYATs33dW4"
      },
      "source": [
        "### Update the weights\n",
        "\n",
        "The simplest update rule used in practice is the Stochastic Gradient\n",
        "Descent (SGD):\n",
        "\n",
        "     weight = weight - learning_rate * gradient\n",
        "\n",
        "We can implement this using simple python code:\n",
        "\n",
        "```python\n",
        "learning_rate = 0.01\n",
        "for f in net.parameters():\n",
        "    f.data.sub_(f.grad.data * learning_rate)\n",
        "```\n",
        "\n",
        "However, as you use neural networks, you'll want to use various different\n",
        "update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc.\n",
        "To enable this, PyTorch has a small package: `torch.optim` that\n",
        "implements all these methods. Using it is very simple:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "Jy4gmWx054ub",
        "outputId": "5aa2c02f-1b04-4896-83c0-2c3515ced413",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.6461, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Create your optimizer\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "# In your training loop:\n",
        "optimizer.zero_grad()             # zero the gradient buffers\n",
        "output = net(input)               # compute the forward pass\n",
        "loss = criterion(output, target)  # compute the loss\n",
        "loss.backward()                   # compute the gradients\n",
        "optimizer.step()                  # update the parameters\n",
        "\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jpw3IUEi54uc"
      },
      "source": [
        "**Important:** Note how gradient buffers had to be manually set to zero using `optimizer.zero_grad()`. This is because gradients are accumulated, so if you don't zero gradients before each `backward()` call, you will begin accumulating gradients from previous forward/backward passes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-IGO5uO54ud"
      },
      "source": [
        "#### Note on eval and train modes\n",
        "\n",
        "**Important**: If you use layers in your network like `torch.nn.Dropout` or `torch.nn.BatchNorm2d` which have different behavior during training and evaluation, you will need to make sure the modules in your network are appropriately set. PyTorch makes this easy with `eval` and `train` methods for any network extending `nn.Module`. Before beginning training you will call `net.train()` to set all modules in the network to train mode, and equivalently before evaluating you should call `net.eval()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qptw28n21n2K"
      },
      "source": [
        "## Training a Classifier\n",
        "\n",
        "Now that you have seen the basics of how to define neural networks, compute losses, and make training updates, you will see how a simple classifier is trained in PyTorch on CIFAR-10."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-jXBv_c15mP"
      },
      "source": [
        "### What about data?\n",
        "\n",
        "Generally, when you have to deal with image, text, audio, or video data,\n",
        "you can use standard python packages that load data into a numpy array.\n",
        "Then you can convert this array into a `torch.*Tensor`.\n",
        "\n",
        "-  For images, packages such as Pillow, OpenCV are useful\n",
        "-  For audio, packages such as scipy and librosa\n",
        "-  For text, either raw Python or Cython based loading, or NLTK and\n",
        "   SpaCy are useful\n",
        "\n",
        "Specifically for vision, we have created a package called\n",
        "`torchvision`, that has data loaders for common datasets such as\n",
        "Imagenet, CIFAR10, MNIST, etc., models for common architectures, and data transformers for images.\n",
        "\n",
        "This provides a huge convenience and avoids writing boilerplate code.\n",
        "\n",
        "For this tutorial, we will use the CIFAR10 dataset.\n",
        "It has the classes: 'airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "'dog', 'frog', 'horse', 'ship', 'truck'. The images in CIFAR-10 are of\n",
        "size $3 \\times 32 \\times 32$, i.e. 3-channel color images of $32 \\times 32$ pixels in size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiDBy41M1_0j"
      },
      "source": [
        "### Training an image classifier\n",
        "\n",
        "We will do the following steps in order:\n",
        "\n",
        "1. Load and normalizing the CIFAR10 training and test datasets using ``torchvision``\n",
        "2. Define a Convolution Neural Network\n",
        "3. Define a loss function\n",
        "4. Train the network on the training data\n",
        "5. Test the network on the test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KbELnF454uk"
      },
      "source": [
        "### 1) Loading and normalizing CIFAR10\n",
        "\n",
        "Using `torchvision`, it’s extremely easy to load CIFAR10.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "0uqKo7VI54ul"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLCMgb2Y54uo"
      },
      "source": [
        "The output of torchvision datasets are `PIL` images of range [0, 1].\n",
        "We transform them to Tensors of normalized range [-1, 1] using the `transforms.ToTensor` and `transforms.Normalize` functions.\n",
        "\n",
        "The [transforms package](https://pytorch.org/vision/stable/transforms.html) has other functions that you might use for **data augmentation**. For example, `torchvision.transforms.RandomResizedCrop` and `torchvision.transforms.RandomHorizontalFlip`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "SgoDH0Yh54uo",
        "outputId": "a113c564-3b8e-452c-c984-3b8278c4269f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 43.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "# Datasets\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=train_transform)\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "# Data loaders\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=4, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=4, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGfPvUSa54uq"
      },
      "source": [
        "Let us show some of the training images, for fun.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "wc0QqWeK54uq",
        "outputId": "57c8f0f5-4ee3-455d-fcdf-400c794d55bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  cat       bird      plane      truck\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def imshow(img):\n",
        "    \"\"\"Function to display an image.\"\"\"\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.axis('off')\n",
        "\n",
        "\n",
        "# Get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "\n",
        "# Show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# Print labels\n",
        "print('      '.join('%5s' % classes[labels[j]] for j in range(4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFVH9GDs54ut"
      },
      "source": [
        "#### Training on GPU\n",
        "\n",
        "Just like how you transfer a Tensor on to the GPU, you transfer the neural\n",
        "net onto the GPU.\n",
        "\n",
        "Let's first define our device as the first visible cuda device if we have\n",
        "CUDA available:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "Tg4nH4kL54ut",
        "outputId": "b2d1acf1-cd23-4833-c504-0e7321984752",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAi1IRlm54uv"
      },
      "source": [
        "If `device` is in fact set to a CUDA device, then these methods will recursively go over all modules and convert their parameters and buffers to CUDA tensors:\n",
        "\n",
        "```python\n",
        "net = net.to(device)\n",
        "```\n",
        "\n",
        "Remember that you will have to send the inputs and targets at every step\n",
        "to the GPU too:\n",
        "\n",
        "```python\n",
        "inputs, labels = inputs.to(device), labels.to(device)\n",
        "```\n",
        "\n",
        "Why don't I notice MASSIVE speedup compared to CPU? Because your network\n",
        "is *realllly* small.\n",
        "\n",
        "**Exercise:** Try increasing the width of your network (argument 2 of\n",
        "the first `nn.Conv2d`, and argument 1 of the second `nn.Conv2d` –\n",
        "they need to be the same number), see what kind of speedup you get."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f3yqmzZ54uv"
      },
      "source": [
        "### 2) Define a Convolution Neural Network\n",
        "\n",
        "Copy the neural network from the Neural Networks section before and modify it to\n",
        "take 3-channel images (instead of 1-channel images as it was defined).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "QFxHduMg54uw"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.out_channels1 = 6\n",
        "        self.out_channels2 = 16\n",
        "        self.conv1 = nn.Conv2d(3, self.out_channels1, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(self.out_channels1, self.out_channels2, 5)\n",
        "        self.fc1 = nn.Linear(self.out_channels2 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.flatten(start_dim=1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNZpQALm54ux"
      },
      "source": [
        "### 3) Define a Loss function and optimizer\n",
        "\n",
        "Let's use a Classification Cross-Entropy loss and SGD with momentum.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "CxS9T6ly54uy"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "#optimizer = optim.Adam(net.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUwlfYEh54u0"
      },
      "source": [
        "### 4) Train the network\n",
        "\n",
        "\n",
        "This is when things start to get interesting.\n",
        "We simply have to loop over our data iterator, and feed the inputs to the\n",
        "network and optimize.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "dwXciSA454u0",
        "outputId": "9692f19b-4787-4e37-b340-735b4c4c03c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,  2000] loss: 2.196\n",
            "iters time: 9.863922357559204\n",
            "[1,  4000] loss: 1.831\n",
            "iters time: 9.048663854598999\n",
            "[1,  6000] loss: 1.661\n",
            "iters time: 10.307585000991821\n",
            "[1,  8000] loss: 1.568\n",
            "iters time: 8.469603061676025\n",
            "[1, 10000] loss: 1.502\n",
            "iters time: 9.12402892112732\n",
            "[1, 12000] loss: 1.472\n",
            "iters time: 9.048671245574951\n",
            "[2,  2000] loss: 1.378\n",
            "iters time: 9.135143041610718\n",
            "[2,  4000] loss: 1.371\n",
            "iters time: 9.556065082550049\n",
            "[2,  6000] loss: 1.326\n",
            "iters time: 8.789440631866455\n",
            "[2,  8000] loss: 1.304\n",
            "iters time: 9.039832830429077\n",
            "[2, 10000] loss: 1.283\n",
            "iters time: 9.205575704574585\n",
            "[2, 12000] loss: 1.258\n",
            "iters time: 8.548995733261108\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "\n",
        "net = net.train()\n",
        "\n",
        "# Loop over the dataset for multiple epochs\n",
        "for epoch in range(1, 3):\n",
        "    running_loss = 0.0\n",
        "    t_s = time.time()\n",
        "\n",
        "    # For each mini-batch...\n",
        "    for i, data in enumerate(trainloader, 1):\n",
        "        # Get the inputs\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 0:  # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch, i, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "            print('iters time:', time.time() - t_s)\n",
        "            t_s = time.time()\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxqpJ27N_23W"
      },
      "source": [
        "Let’s quickly save our trained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "65bQ4MFR_y52"
      },
      "outputs": [],
      "source": [
        "PATH = './cifar_net.pth'\n",
        "torch.save(net.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdGUM74bAJn6"
      },
      "source": [
        "We can load back in a saved model with the following: (note: saving and re-loading the model wasn’t necessary here, we only did it to illustrate how to do so):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "QuoPRDvWAUYs",
        "outputId": "b879768a-6f63-480c-9a9e-fd031bd10e7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "net = Net().to(device)\n",
        "net.load_state_dict(torch.load(PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0X8mDOw54u1"
      },
      "source": [
        "### 5) Test the network on the test data\n",
        "\n",
        "We have trained the network for 2 passes over the training dataset.\n",
        "But we need to check if the network has learned anything at all.\n",
        "\n",
        "We will check this by predicting the class label that the neural network\n",
        "outputs, and checking it against the ground-truth. If the prediction is\n",
        "correct, we add the sample to the list of correct predictions.\n",
        "\n",
        "Okay, first step. Let us display an image from the test set to get familiar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "AxuALLU054u2",
        "outputId": "72370906-6815-4e8c-a5f6-cf75d174256e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground truth:\n",
            "  cat       ship       ship      plane\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "dataiter = iter(testloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# print images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "print('Ground truth:')\n",
        "print('      '.join('%5s' % classes[labels[j]] for j in range(4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17PzSHOh54u4"
      },
      "source": [
        "Okay, now let us see what the neural network thinks these examples above are:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "YUalV9gM54u4"
      },
      "outputs": [],
      "source": [
        "net = net.eval()\n",
        "\n",
        "outputs = net(images.to(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXLcTThZ54u6"
      },
      "source": [
        "The outputs are energies for the 10 classes.\n",
        "Higher the energy for a class, the more the network\n",
        "thinks that the image is of the particular class.\n",
        "So, let's get the index of the highest energy:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "BttfSl0_54u7",
        "outputId": "c39a363e-9a70-4212-e4ee-91e900bd83a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted:\n",
            "  cat       ship      truck      plane\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "print('Predicted:')\n",
        "print('      '.join('%5s' % classes[predicted[j]] for j in range(4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQOFenEC54u-"
      },
      "source": [
        "The results seem pretty good.\n",
        "\n",
        "Let us look at how the network performs on the whole dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "iKFpv3i354u-",
        "outputId": "121a2c0c-9226-424a-fb8e-74c1b740c3e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 55%\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d%%' % (\n",
        "    100 * correct / total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwJ2a57954vA"
      },
      "source": [
        "That looks waaay better than chance, which is 10% accuracy (randomly picking\n",
        "a class out of 10 classes).\n",
        "Seems like the network learned something.\n",
        "\n",
        "Hmmm, what are the classes that performed well, and the classes that did\n",
        "not perform well:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "6kRdd7As54vB",
        "outputId": "53dedf81-a03c-4d12-c69f-1546eb55ccf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of plane: 49%\n",
            "Accuracy of   car: 48%\n",
            "Accuracy of  bird: 29%\n",
            "Accuracy of   cat: 42%\n",
            "Accuracy of  deer: 52%\n",
            "Accuracy of   dog: 48%\n",
            "Accuracy of  frog: 63%\n",
            "Accuracy of horse: 68%\n",
            "Accuracy of  ship: 75%\n",
            "Accuracy of truck: 76%\n"
          ]
        }
      ],
      "source": [
        "class_correct = [0] * 10\n",
        "class_total = [0] * 10\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(4):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    print('Accuracy of %5s: %2d%%' % (\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3V4E0pI54vE"
      },
      "source": [
        "### Other information\n",
        "\n",
        "How to write data loading code in PyTorch: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "\n",
        "More details on saving and loading models: https://pytorch.org/tutorials/beginner/saving_loading_models.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw33n0lVaflD"
      },
      "source": [
        "## Other Tips and Helpful Functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-XdYBHho1b1"
      },
      "source": [
        "### Tips for debugging\n",
        "\n",
        "Checklist for common PyTorch mistakes:\n",
        "\n",
        "* Did you set `shuffle=True` in your train dataloader?\n",
        "* Did you properly set `net.train()` and `net.eval()` in your training and evaluation code?\n",
        "* Did you call `zero_grad()` in your training loop before `.backward()` to prevent gradients from accumulating?\n",
        "\n",
        "Other tips:\n",
        "* Have you visualized your loaded images? This is the best way to catch data loader issues.\n",
        "* If you are getting a CUDA out of memory error, first try decreasing the batch size. If you are still getting the same error, your network may simply be too large, or you could be accidentally allocating a large array in memory.\n",
        "* If the GPU memory is full, first try clearing the outputs and restarting the kernel. If that does not work, manually clear the GPU memory using `torch.cuda.empty_cache()`.\n",
        "* Getting CUDA errors that are hard to understand? Sometimes error messages will be simpler if you switch your network to cpu memory to debug the forward and backward passes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IM517FCeGGmh"
      },
      "source": [
        "### Pretrained models\n",
        "\n",
        "PyTorch provides easy access to load many pretrained models. You can find a wide variety of vision models pretrained for different tasks in the `torchvision` package: https://pytorch.org/vision/stable/models.html\n",
        "\n",
        "To load a ResNet50 model pretrained on ImageNet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "U-02dfgAagK3",
        "outputId": "d7a288f4-db5b-48b3-f9f9-ee429a27e220",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 169MB/s]\n"
          ]
        }
      ],
      "source": [
        "from torchvision.models import resnet50\n",
        "\n",
        "resnet = resnet50(pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ryw7NzWLs-_"
      },
      "source": [
        "It's common that you may want to finetune some or all of the weights in a pretrained model. You can check here for more details on how to do this: https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
        "\n",
        "\n",
        "There are even more pretrained models available on the PyTorch Hub: https://pytorch.org/hub/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv9zqWFlR9jx"
      },
      "source": [
        "### More Tensor operations\n",
        "\n",
        "The `torch.einsum` function offers a compact way to express various matrix transformations and products. Many of common matrix and vector computations can be easily expressed elegantly with a call to this function.\n",
        "\n",
        "Some simple examples are below, but you can find many more example einsum operations in this helpful blog post: https://rockt.github.io/2018/04/30/einsum\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "nls4hhaUZggy",
        "outputId": "963ca4e3-9289-4a03-a905-a7023f40f946",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x:  tensor([[0, 1, 2],\n",
            "        [3, 4, 5]])\n",
            "tensor([[0, 3],\n",
            "        [1, 4],\n",
            "        [2, 5]])\n",
            "tensor([ 3, 12])\n",
            "tensor(15)\n"
          ]
        }
      ],
      "source": [
        "x = torch.arange(6).reshape(2, 3)\n",
        "print('x: ', x)\n",
        "\n",
        "# matrix transpose\n",
        "out = torch.einsum('ij->ji', [x])\n",
        "print(out)\n",
        "\n",
        "# sum all the rows in a matrix\n",
        "out = torch.einsum('ij->i', [x])\n",
        "print(out)\n",
        "\n",
        "# sum all the values in a matrix\n",
        "out = torch.einsum('ij->', [x])\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DU8WXqJbUUZ"
      },
      "source": [
        "Operations on two matrices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "M2vLY0r0aygj",
        "outputId": "9c0dd4b9-f0b9-4159-f025-72fb4b8b12c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x:  tensor([[0, 1, 2],\n",
            "        [3, 4, 5],\n",
            "        [6, 7, 8]])\n",
            "y:  tensor([[0, 1, 2],\n",
            "        [3, 4, 5],\n",
            "        [6, 7, 8]])\n",
            "tensor([[ 0,  1,  4],\n",
            "        [ 9, 16, 25],\n",
            "        [36, 49, 64]])\n",
            "tensor([[ 15,  18,  21],\n",
            "        [ 42,  54,  66],\n",
            "        [ 69,  90, 111]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.arange(9).reshape(3, 3)\n",
        "y = torch.arange(9).reshape(3, 3)\n",
        "print('x: ', x)\n",
        "print('y: ', y)\n",
        "\n",
        "# element-wise multiplication\n",
        "out = torch.einsum('ij,ij->ij', [x, y])\n",
        "print(out)\n",
        "\n",
        "# matrix multiplication\n",
        "out = torch.einsum('ik,kj->ij', [x, y])\n",
        "print(out)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "deyo_env",
      "language": "python",
      "name": "deyo_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}